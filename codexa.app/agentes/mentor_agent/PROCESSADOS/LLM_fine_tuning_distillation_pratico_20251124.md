# Fine-Tuning e Knowledge Distillation: Guia PrÃ¡tico para LLMs

**Categoria**: llm_advanced
**Assunto**: fine_tuning_distillation
**NÃ­vel**: avanÃ§ado
**AplicaÃ§Ã£o**: quando_customizar_llms_producao
**Tags**: sft, dpo, knowledge-distillation, fine-tuning, ml-pratico
**Quality Score**: 0.92/1.0

---

## RESUMO EXECUTIVO

Este documento ensina **como treinar LLMs customizados** via 3 tÃ©cnicas principais: SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization) e Knowledge Distillation. Foco 100% prÃ¡tico: quando usar, como implementar, code snippets executÃ¡veis, e trade-offs reais para produÃ§Ã£o.

**Para quem**: Desenvolvedores que precisam customizar LLMs para casos especÃ­ficos (agentes especializados, domÃ­nios proprietÃ¡rios, otimizaÃ§Ã£o custo/performance).

**Resultado esperado**: Criar versÃµes customizadas de LLMs que:
- Performam 2-5x melhor em seu domÃ­nio especÃ­fico
- Custam 10-50x menos em produÃ§Ã£o (via distillation)
- Evoluem continuamente com feedback de usuÃ¡rios

---

## CONCEITOS-CHAVE

### 1. SFT (Supervised Fine-Tuning): Ensinar o CERTO

**O que Ã©**: Treinar LLM com pares `(input, output_correto)` para especializar em tarefa especÃ­fica.

**MetÃ¡fora**: Professor corrigindo exercÃ­cios com gabarito vermelho.

**Quando usar**:
- âœ… VocÃª tem 1K-100K exemplos rotulados de qualidade
- âœ… Tarefa tem "resposta certa" objetiva (traduÃ§Ã£o, extraÃ§Ã£o, classificaÃ§Ã£o)
- âœ… LLM base nÃ£o performa bem na tarefa mesmo com prompting avanÃ§ado
- âœ… VocÃª precisa de consistÃªncia alta (nÃ£o pode tolerar variaÃ§Ã£o)

**Quando NÃƒO usar**:
- âŒ Tarefa Ã© subjetiva (tom, estilo, preferÃªncias)
- âŒ VocÃª tem <500 exemplos (overfitting garantido)
- âŒ LLM base jÃ¡ performa 90%+ com bom prompt
- âŒ Custo/tempo de fine-tuning nÃ£o vale ganho marginal

**Code PrÃ¡tico (PyTorch + HuggingFace)**:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset

# 1. PREPARAR DADOS
training_data = [
    {
        "input": "Resuma: [texto longo sobre IA...]",
        "output": "IA transforma indÃºstrias via automaÃ§Ã£o de tarefas complexas."
    },
    {
        "input": "Classifique sentimento: 'Adorei este produto!'",
        "output": "Positivo"
    },
    # ... 10K exemplos mÃ­nimo
]

# Converter para formato HuggingFace
dataset = Dataset.from_dict({
    "text": [f"{ex['input']}\n\n{ex['output']}" for ex in training_data]
})

# 2. CARREGAR MODELO BASE
model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"  # Modelo pequeno, rÃ¡pido
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. TOKENIZAR
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 4. CONFIGURAR TREINAMENTO
training_args = TrainingArguments(
    output_dir="./modelo_finetuned",
    num_train_epochs=3,              # 3 epochs suficiente para fine-tune
    per_device_train_batch_size=4,   # Ajustar conforme GPU
    learning_rate=2e-5,               # LR baixo para nÃ£o "esquecer" pre-training
    weight_decay=0.01,
    logging_steps=100,
    save_steps=1000,
    eval_strategy="steps",
    eval_steps=500,
    warmup_steps=200,
    fp16=True,                        # Mixed precision = 2x mais rÃ¡pido
)

# 5. TREINAR
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()

# 6. SALVAR MODELO CUSTOMIZADO
model.save_pretrained("./modelo_finetuned_final")
tokenizer.save_pretrained("./modelo_finetuned_final")
```

**Hyperparameters CrÃ­ticos**:
- `learning_rate`: 1e-5 a 5e-5 (menor = mais conservador, evita "esquecer" base)
- `num_epochs`: 2-5 (mais = overfitting, menos = underfitting)
- `batch_size`: MÃ¡ximo que cabe na GPU (maior = mais estÃ¡vel)
- `warmup_steps`: 10-20% do total (aquece gradualmente)

**ValidaÃ§Ã£o ObrigatÃ³ria**:
```python
# Sempre separe train/validation/test
# 80% train, 10% validation, 10% test

# MÃ©tricas essenciais:
# - Perplexity (quanto menor, melhor)
# - AcurÃ¡cia na tarefa especÃ­fica
# - Human evaluation (amostra de 100 outputs)

# Red flags:
# - Train loss cai mas val loss sobe = OVERFITTING
# - Model repete outputs = COLLAPSE
# - Perda de capacidade em outras tarefas = CATASTROPHIC FORGETTING
```

---

### 2. DPO (Direct Preference Optimization): Ensinar o MELHOR

**O que Ã©**: Treinar LLM com pares `(prompt, resposta_boa, resposta_ruim)` para alinhar com preferÃªncias humanas **sem** reward model intermediÃ¡rio.

**MetÃ¡fora**: Sommelier escolhendo "este vinho > aquele vinho" sem precisar dar nota numÃ©rica.

**DiferenÃ§a vs RLHF**:
```
RLHF (Complexo):
  Dados â†’ Treina Reward Model â†’ RL via PPO â†’ Modelo alinhado
  Problema: Reward model pode ser instÃ¡vel, RL Ã© difÃ­cil de calibrar

DPO (Simples):
  Dados â†’ DPO direto â†’ Modelo alinhado
  Vantagem: Sem reward model, mais estÃ¡vel, mais rÃ¡pido
```

**Quando usar**:
- âœ… Tarefa tem mÃºltiplas respostas "corretas" mas algumas sÃ£o **melhores**
- âœ… VocÃª tem feedback humano (thumbs up/down, rankings)
- âœ… Precisa alinhar tom, estilo, helpful vs harmful
- âœ… SFT jÃ¡ funcionou mas precisa de refinamento

**Quando NÃƒO usar**:
- âŒ Tarefa tem resposta binÃ¡ria certa/errada (use SFT)
- âŒ Feedback humano Ã© inconsistente/ruidoso
- âŒ VocÃª tem <1K pares de preferÃªncia

**Code PrÃ¡tico (TRL Library)**:

```python
from trl import DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset

# 1. PREPARAR DADOS DPO
dpo_data = [
    {
        "prompt": "Escreva email profissional pedindo reuniÃ£o",
        "chosen": "Prezado Sr. JoÃ£o,\n\nGostaria de agendar reuniÃ£o...",  # âœ… Melhor
        "rejected": "Oi JoÃ£o, blz? Vamos marcar papo? Vlw"              # âŒ Pior
    },
    {
        "prompt": "Resuma este artigo sobre IA: [...]",
        "chosen": "IA revoluciona indÃºstrias atravÃ©s de...",            # âœ… Claro
        "rejected": "IA tipo assim faz umas coisas legais..."           # âŒ Vago
    },
    # ... 5K-50K pares mÃ­nimo
]

dataset = Dataset.from_dict({
    "prompt": [ex["prompt"] for ex in dpo_data],
    "chosen": [ex["chosen"] for ex in dpo_data],
    "rejected": [ex["rejected"] for ex in dpo_data],
})

# 2. CARREGAR MODELO (jÃ¡ fine-tuned com SFT idealmente)
model = AutoModelForCausalLM.from_pretrained("./modelo_finetuned_final")
ref_model = AutoModelForCausalLM.from_pretrained("./modelo_finetuned_final")  # Frozen
tokenizer = AutoTokenizer.from_pretrained("./modelo_finetuned_final")

# 3. CONFIGURAR DPO
from trl import DPOConfig

dpo_config = DPOConfig(
    output_dir="./modelo_dpo",
    num_train_epochs=1,              # DPO geralmente 1 epoch suficiente
    per_device_train_batch_size=2,   # Precisa carregar 2 modelos = menos batch
    learning_rate=5e-7,               # Muito menor que SFT
    beta=0.1,                         # ForÃ§a do KL penalty (0.1 padrÃ£o)
    logging_steps=10,
)

# 4. TREINAR DPO
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

trainer.train()

# 5. SALVAR MODELO ALINHADO
model.save_pretrained("./modelo_dpo_final")
```

**Hyperparameter Beta**:
```python
# Beta controla trade-off entre seguir preferÃªncias vs manter capacidade original

beta = 0.01   # Muito permissivo, quase nÃ£o muda
beta = 0.1    # PadrÃ£o, equilÃ­brio bom
beta = 0.5    # Agressivo, forte alinhamento mas pode perder capabilities
beta = 1.0    # Muito agressivo, risco de overfitting

# Regra: Comece com 0.1, aumente se model nÃ£o alinha suficiente
```

**Pipeline Completo (SFT â†’ DPO)**:
```python
# FASE 1: SFT com dados rotulados
sft_model = finetune_sft(
    base="SmolLM2-1.7B",
    data=labeled_examples,  # 10K exemplos com gabarito
)

# FASE 2: Coletar feedback em produÃ§Ã£o
# UsuÃ¡rios marcam outputs como ğŸ‘ ou ğŸ‘

# FASE 3: DPO com preferÃªncias
dpo_model = align_dpo(
    base=sft_model,
    data=preference_pairs,  # 5K pares (chosen/rejected)
)

# FASE 4: Deploy versÃ£o alinhada
deploy(dpo_model)

# FASE 5: Repeat mensalmente (continuous learning loop)
```

---

### 3. Knowledge Distillation: Comprimir INTELIGÃŠNCIA

**O que Ã©**: Treinar modelo **pequeno** (aluno) para imitar modelo **grande** (professor), mantendo 70-90% da qualidade com 10-50x menos custo.

**MetÃ¡fora**: Mestre (GPT-4) ensinando aprendiz (SmolLM) â€” aprendiz nunca serÃ¡ mestre, mas aprende 80% das liÃ§Ãµes por 2% do preÃ§o.

**Quando usar**:
- âœ… Professor (GPT-4/Claude) Ã© muito caro/lento para produÃ§Ã£o
- âœ… VocÃª precisa rodar 1M+ chamadas/dia
- âœ… LatÃªncia Ã© crÃ­tica (<100ms)
- âœ… VocÃª aceita 10-20% de perda de qualidade por 50x reduÃ§Ã£o de custo

**Trade-offs Reais**:
```
Professor (GPT-4):
  Qualidade: 10/10
  Custo: $0.03/1K tokens = $30/1M tokens
  LatÃªncia: 2-5 segundos

Aluno Destilado (SmolLM 1.7B):
  Qualidade: 7-8/10
  Custo: $0.0005/1K tokens = $0.50/1M tokens (60x mais barato!)
  LatÃªncia: 50-200ms (10-25x mais rÃ¡pido!)

Vale a pena? DEPENDE do caso de uso.
```

**Casos Ideais para Distillation**:
1. **Chatbots de suporte**: Respostas repetitivas, padrÃµes claros
2. **ClassificaÃ§Ã£o em escala**: Sentiment analysis, topic tagging
3. **ExtraÃ§Ã£o de dados**: Parsing de documentos estruturados
4. **GeraÃ§Ã£o de variaÃ§Ãµes**: Product descriptions, email templates

**Casos RUINS para Distillation**:
1. **RaciocÃ­nio complexo**: MatemÃ¡tica avanÃ§ada, lÃ³gica multi-step
2. **Criatividade alta**: Copywriting premium, storytelling
3. **DomÃ­nio super-especializado**: QuestÃµes mÃ©dicas/legais raras

**Code PrÃ¡tico (Soft Labels)**:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. CARREGAR PROFESSOR E ALUNO
professor = AutoModelForCausalLM.from_pretrained("gpt-4")  # HipotÃ©tico
aluno = AutoModelForCausalLM.from_pretrained("SmolLM2-1.7B")

tokenizer = AutoTokenizer.from_pretrained("SmolLM2-1.7B")

# 2. CORPUS DE TREINO (nÃ£o precisa ser rotulado!)
corpus = load_unlabeled_documents(n=100_000)  # 100K docs

# 3. DISTILLATION LOOP
temperature = 2.0  # "Suaviza" probabilidades do professor

for doc in corpus:
    tokens = tokenizer(doc, return_tensors="pt")

    # Professor gera SOFT LABELS (distribuiÃ§Ã£o de probabilidades)
    with torch.no_grad():
        professor_logits = professor(**tokens).logits / temperature
        professor_probs = torch.softmax(professor_logits, dim=-1)

    # Aluno tenta imitar distribuiÃ§Ã£o
    aluno_logits = aluno(**tokens).logits / temperature
    aluno_probs = torch.softmax(aluno_logits, dim=-1)

    # Loss = KL Divergence (distÃ¢ncia entre distribuiÃ§Ãµes)
    kl_loss = torch.nn.functional.kl_div(
        torch.log(aluno_probs),
        professor_probs,
        reduction='batchmean'
    )

    # Backprop no aluno
    kl_loss.backward()
    optimizer.step()

# 4. VALIDAR ALUNO
# Compare outputs de professor vs aluno em test set
# MÃ©tricas: BLEU, ROUGE, Human eval
```

**Soft Labels vs Hard Labels**:
```python
# HARD LABEL (SFT tradicional)
# BinÃ¡rio: certo ou errado
input = "O gato"
label = "dorme"  # Ãšnica resposta aceita

# SOFT LABEL (Distillation)
# DistribuiÃ§Ã£o de probabilidades do professor
input = "O gato"
soft_label = {
    "dorme": 0.45,    # Alta probabilidade
    "come": 0.20,     # MÃ©dia probabilidade
    "pula": 0.15,     # Baixa probabilidade
    "mia": 0.10,      # Baixa
    "voa": 0.001      # Quase zero
}

# Vantagem: Aluno aprende NUANCES
# "dorme" Ã© melhor, mas "come" nÃ£o Ã© absurdo
# "voa" Ã© virtualmente impossÃ­vel
```

**Pipeline Completo de Distillation**:

```python
from distillation_lib import DistillationPipeline  # HipotÃ©tico

# FASE 1: Definir Professor e Aluno
pipeline = DistillationPipeline(
    teacher="gpt-4-turbo",           # Ou Claude Sonnet 4
    student="SmolLM2-1.7B-base",     # Modelo leve
    task="summarization"             # Tarefa especÃ­fica
)

# FASE 2: Coletar dados nÃ£o rotulados
corpus = load_domain_documents(
    domain="ecommerce",
    n=50_000
)

# FASE 3: Professor gera labels automÃ¡ticos
labeled_corpus = pipeline.teacher_label(
    corpus=corpus,
    output_format="summary"
)

# FASE 4: Treinar aluno com soft labels
student_model = pipeline.distill(
    data=labeled_corpus,
    epochs=10,
    temperature=2.0,
    alpha=0.7  # 70% soft labels, 30% hard labels
)

# FASE 5: Validar tradeoff qualidade/custo
metrics = pipeline.evaluate(
    test_set=validation_data,
    metrics=["quality", "latency", "cost"]
)

print(metrics)
# {
#   "quality": 0.82,        # 82% da qualidade do professor
#   "latency": "120ms",     # 15x mais rÃ¡pido
#   "cost_reduction": "52x" # 52x mais barato
# }

# FASE 6: Deploy se trade-off aceitÃ¡vel
if metrics["quality"] > 0.75 and metrics["cost_reduction"] > 20:
    deploy_student(student_model)
```

---

## COMO APLICAR: Workflow Completo

### Use Case Real: Agente AnÃºncio (CODEXA)

**Problema**: Anuncio_Agent usa GPT-4 para gerar copy de marketplace. Custo = $500/mÃªs para 100K geraÃ§Ãµes.

**SoluÃ§Ã£o**: SFT â†’ DPO â†’ Distillation

**FASE 1: SFT (Especializar)**
```python
# Coletar 10K exemplos reais de anÃºncios high-performing
training_data = [
    {
        "input": "Produto: TÃªnis Nike Air Max\nEspecs: Tamanho 42, Preto...",
        "output": "TÃªnis Nike Air Max Preto 42 | Conforto MÃ¡ximo | Entrega RÃ¡pida SP"
    },
    # ... 10K exemplos
]

# Fine-tune SmolLM
anuncio_sft = finetune_sft(
    base="SmolLM2-1.7B",
    data=training_data,
    epochs=3
)

# Resultado: Modelo especializado em copy de marketplace
```

**FASE 2: DPO (Alinhar com ConversÃ£o)**
```python
# Coletar feedback real: quais anÃºncios converteram melhor?
dpo_data = [
    {
        "prompt": "Gere anÃºncio para: TÃªnis Nike...",
        "chosen": "TÃ­tulo otimizado que converteu 8%",    # A/B test winner
        "rejected": "TÃ­tulo genÃ©rico que converteu 2%"   # A/B test loser
    },
    # ... 5K pares
]

# Alinhar com conversÃ£o real
anuncio_dpo = align_dpo(
    base=anuncio_sft,
    data=dpo_data
)

# Resultado: Modelo que gera copy que CONVERTE
```

**FASE 3: Distillation (Otimizar Custo)**
```python
# Professor = anuncio_dpo (jÃ¡ especializado)
# Aluno = SmolLM-360M (4x menor)

anuncio_distilled = distill(
    teacher=anuncio_dpo,
    student="SmolLM-360M-base",
    corpus=unlabeled_product_specs  # 100K specs nÃ£o rotulados
)

# Resultado:
# - Qualidade: 88% do modelo DPO
# - Custo: $10/mÃªs (50x reduÃ§Ã£o!)
# - LatÃªncia: 80ms (20x mais rÃ¡pido)
```

**ROI Final**:
- Investimento: 3 semanas de dev + $200 em compute
- Economia anual: $500/mÃªs â†’ $10/mÃªs = $5.880/ano
- Payback: <1 mÃªs

---

## ARMADILHAS COMUNS

### âŒ Erro 1: Fine-tune com dados ruins
```python
# RUIM: Dados sujos, inconsistentes
bad_data = [
    {"input": "resumo isso", "output": "resumo de algo"},  # Vago
    {"input": "Traduza: hello", "output": "olÃ¡"},          # Ok
    {"input": "classifique", "output": "positivo"},        # Sem contexto
]

# BOM: Dados limpos, consistentes, diversos
good_data = [
    {"input": "Resuma em 2 frases: [texto completo 200 palavras]",
     "output": "Frase 1 clara. Frase 2 clara."},
    {"input": "Traduza para portuguÃªs formal: 'Hello, how are you?'",
     "output": "OlÃ¡, como vocÃª estÃ¡?"},
]

# REGRA: 1K exemplos excelentes > 10K exemplos medÃ­ocres
```

### âŒ Erro 2: Overfitting (memorizar em vez de generalizar)
```python
# Sinais de overfitting:
# - Train loss: 0.05 (excelente)
# - Val loss: 2.5 (pÃ©ssimo)
# - Model repete exatamente exemplos de treino
# - Performa mal em inputs ligeiramente diferentes

# SoluÃ§Ã£o:
# - Reduzir epochs (3 â†’ 1)
# - Aumentar dropout (0.1 â†’ 0.3)
# - Adicionar regularizaÃ§Ã£o (weight_decay=0.01)
# - Aumentar diversidade de dados
```

### âŒ Erro 3: Escolher tarefa errada para distillation
```python
# RUIM para distillation:
# - RaciocÃ­nio complexo (ex: provas matemÃ¡ticas)
# - Tarefas ultra-especializadas (ex: diagnÃ³stico mÃ©dico raro)
# - Criatividade alta (ex: roteiro de filme)

# BOM para distillation:
# - ClassificaÃ§Ã£o (ex: sentiment analysis)
# - ExtraÃ§Ã£o (ex: NER, parsing)
# - GeraÃ§Ã£o de variaÃ§Ãµes (ex: paraphrasing)
# - Tarefas repetitivas com padrÃµes claros
```

---

## QUANDO USAR: Decision Tree

```
Preciso customizar LLM?
  â”œâ”€ LLM base + bom prompt jÃ¡ funciona 90%+?
  â”‚   â””â”€ NÃƒO FINE-TUNE. Use prompting avanÃ§ado.
  â”‚
  â”œâ”€ Tenho 1K-100K exemplos rotulados?
  â”‚   â”œâ”€ SIM â†’ Use SFT
  â”‚   â””â”€ NÃƒO â†’ Colete dados primeiro
  â”‚
  â”œâ”€ Tarefa Ã© objetiva (resposta certa) ou subjetiva (preferÃªncia)?
  â”‚   â”œâ”€ Objetiva â†’ SFT suficiente
  â”‚   â””â”€ Subjetiva â†’ SFT + DPO
  â”‚
  â””â”€ Custo/latÃªncia sÃ£o crÃ­ticos?
      â”œâ”€ SIM â†’ ApÃ³s SFT/DPO, faÃ§a Distillation
      â””â”€ NÃƒO â†’ Deploy modelo fine-tuned diretamente
```

---

## RELACIONADO

- Ver tambÃ©m: LLM_prompting_best_practices_20251120.md (quando prompting basta)
- Ver tambÃ©m: LLM_context_management_strategies_20251120.md (otimizar context window)
- Ver tambÃ©m: MULTIAGENT_arquitetura_sistemas_20251120.md (agents especializados via fine-tune)

---

**Fonte**:
- KNOWLEDGE_DISTILLATION_STRATEGY.md
- 04_CONHECIMENTO_APRENDIZADO_META.md
- SmolLM Training Playbook (HuggingFace)
**Processado**: 2025-11-24
**Quality Score**: 0.92/1.0
