# Context Management: Maximizando o Window Limit de LLMs

**Categoria**: llm_optimization
**Qualidade**: 0.90/1.00
**Data**: 20251120

## Conte√∫do

### O Problema do Context Window (A Parede Invis√≠vel)

**Realidade dura**: Todo LLM tem limite de tokens que processa por request. Exceder esse limite = perda de informa√ß√£o ou erro.

- Claude Sonnet 4: ~200k tokens (~150k palavras)
- GPT-4 Turbo: ~128k tokens (~96k palavras)
- [OPEN_VARIABLE: modelo_alternativo]: ~[OPEN_VARIABLE: limite_tokens] tokens

**Analogia**: Imagine um humano que s√≥ consegue lembrar dos √∫ltimos 10 minutos de conversa. Informa√ß√£o antes disso simplesmente desaparece. LLMs t√™m problema similar, s√≥ que medido em tokens.

**Por que isso importa**: Em workflows complexos (multi-agent, sess√µes longas, documenta√ß√£o extensa), voc√™ SEMPRE bate no limite se n√£o gerenciar contexto proativamente.

### As 4 Estrat√©gias Fundamentais

#### **Estrat√©gia 1: Compression (Comprimir sem Perder Ess√™ncia)**

**Princ√≠pio**: Transmitir m√°xima informa√ß√£o com m√≠nimo de tokens.

**T√©cnicas**:

1. **Summarization Agressiva**
```
ORIGINAL (2000 tokens):
[Documento t√©cnico completo de 15 p√°ginas]

COMPRESSED (300 tokens):
[Bullet points com apenas fatos essenciais + estrutura key-value]

Example:
{
  "produto": "Notebook Dell Inspiron 15",
  "specs_critical": ["i7-12700H", "16GB RAM", "512GB SSD"],
  "target_audience": "Profissionais remotos 25-45 anos",
  "price_range": "R$ 3500-4200",
  "differentiators": ["Garantia 3 anos", "Suporte 24/7 Brasil"]
}
```

**Ganho**: 85% redu√ß√£o de tokens mantendo 95%+ da informa√ß√£o acion√°vel.

2. **Eliminate Redundancy**
```
‚ùå RUIM (redundante):
"O produto √© uma cadeira gamer. Esta cadeira gamer possui apoio lombar.
O apoio lombar da cadeira gamer √© ajust√°vel. A cadeira gamer com apoio
lombar ajust√°vel √© ideal para gamers."

‚úÖ BOM (conciso):
"Cadeira gamer: apoio lombar ajust√°vel, ideal para sess√µes longas."
```

**Ganho**: 70% redu√ß√£o de tokens, zero perda de informa√ß√£o.

3. **Structured Data over Natural Language**
```
‚ùå RUIM (verbose):
"O cliente Jo√£o Silva, email joao@email.com, telefone 11-99999-9999,
comprou o produto X por R$ 299 no dia 15 de novembro de 2024."

‚úÖ BOM (estruturado):
```json
{"cliente": "Jo√£o Silva", "email": "joao@email.com", "tel": "11-99999-9999",
 "produto": "X", "valor": 299, "data": "2024-11-15"}
```

**Ganho**: 40% redu√ß√£o de tokens + mais f√°cil de parsear.

#### **Estrat√©gia 2: Chunking (Dividir e Conquistar)**

**Princ√≠pio**: Quebrar tarefas grandes em subtasks menores, cada uma com context window pr√≥prio.

**Pattern: Sequential Chunking**
```
Tarefa grande: Analisar 50 p√°ginas de pesquisa de mercado

Chunk 1 (p√°ginas 1-10) ‚Üí LLM gera: [SUMMARY_1]
Chunk 2 (p√°ginas 11-20) ‚Üí LLM gera: [SUMMARY_2]
Chunk 3 (p√°ginas 21-30) ‚Üí LLM gera: [SUMMARY_3]
Chunk 4 (p√°ginas 31-40) ‚Üí LLM gera: [SUMMARY_4]
Chunk 5 (p√°ginas 41-50) ‚Üí LLM gera: [SUMMARY_5]

Final aggregation:
Input: [SUMMARY_1] + [SUMMARY_2] + [SUMMARY_3] + [SUMMARY_4] + [SUMMARY_5]
‚Üí LLM gera: [CONSOLIDATED_REPORT]
```

**Vantagem**: Processa informa√ß√£o ilimitada, latency linear (n√£o exponencial).

**Pattern: Hierarchical Chunking**
```
Level 1: Chunk em se√ß√µes tem√°ticas
Section A ‚Üí [SUMMARY_A]
Section B ‚Üí [SUMMARY_B]
Section C ‚Üí [SUMMARY_C]

Level 2: Agregar summaries
[SUMMARY_A] + [SUMMARY_B] + [SUMMARY_C] ‚Üí [MEGA_SUMMARY]

Level 3 (se necess√°rio): Refinar
[MEGA_SUMMARY] + specific_question ‚Üí [FINAL_ANSWER]
```

**Exemplo CODEXA (Pesquisa Agent)**:
```
User: "Pesquise mercado para notebook gaming"

Chunking strategy:
1. Chunk concorrentes Mercado Livre ‚Üí [REPORT_ML]
2. Chunk concorrentes Shopee ‚Üí [REPORT_SHOPEE]
3. Chunk keywords Google Trends ‚Üí [REPORT_KEYWORDS]
4. Chunk pricing analysis ‚Üí [REPORT_PRICING]

Aggregation:
[REPORT_ML] + [REPORT_SHOPEE] + [REPORT_KEYWORDS] + [REPORT_PRICING]
‚Üí [CONSOLIDATED_MARKET_RESEARCH]
```

#### **Estrat√©gia 3: Selective Loading (Carregar Apenas o Necess√°rio)**

**Princ√≠pio**: N√£o carregue TUDO no contexto. Carregue apenas o que √© relevante para a tarefa ATUAL.

**Anti-Pattern**:
```python
# RUIM: Carrega 20 arquivos de conhecimento sempre
context = ""
for file in iso_vectorstore:
    context += read_file(file)  # 200k tokens!

llm_call(prompt + context)  # BOOM! Context overflow
```

**Pattern Correto: Discovery-First**:
```python
# BOM: Busca sem√¢ntica para carregar apenas relevante
query = extract_query_from_user_input(user_message)
relevant_files = semantic_search(query, iso_vectorstore, top_k=3)

context = ""
for file in relevant_files:  # Apenas 3 arquivos mais relevantes
    context += read_file(file)  # ~20k tokens

llm_call(prompt + context)  # ‚úÖ Within limits
```

**Exemplo CODEXA (Mentor Agent)**:
```
User: "Como otimizar t√≠tulos no Mercado Livre?"

Semantic search em catalogo.json:
Query: "otimizar t√≠tulos mercado livre"
‚Üí Match 1: marketplace_titulos_seo_20251113.md (score: 0.92)
‚Üí Match 2: marketplace_keywords_strategy_20251108.md (score: 0.85)
‚Üí Match 3: copywriting_seo_fundamentals_20251015.md (score: 0.78)

Load ONLY these 3 files (not all 100+ files in PROCESSADOS/)
‚Üí Context: ~15k tokens
‚Üí Room left for: prompt (5k) + conversation history (10k) + output (10k) ‚úÖ
```

**Tools for Semantic Search**:
- [OPEN_VARIABLE: embedding_model] (ex: text-embedding-ada-002, voyage-2)
- [OPEN_VARIABLE: vector_db] (ex: Chroma, Pinecone, FAISS local)

#### **Estrat√©gia 4: Context Rotation (Girar Contexto Conforme Necessidade)**

**Princ√≠pio**: Em sess√µes longas, "esque√ßa" partes antigas do contexto para dar espa√ßo ao novo.

**Pattern: Sliding Window**
```
Conversation com 20 mensagens:

Tokens por mensagem m√©dia: 500
Total: 20 √ó 500 = 10,000 tokens

Se context limit √© 50k:
- √öltimas 15 mensagens: KEEP (7,500 tokens)
- Mensagens 1-5: DROP (economiza 2,500 tokens)

Summary das mensagens dropadas:
"Conversa inicial sobre pesquisa de produto X, decidiu focar em nicho Y."
(50 tokens)

Total context: 7,500 + 50 + prompt + knowledge = ~20k ‚úÖ
```

**Pattern: Importance-Based Retention**
```python
def should_keep_message(msg, current_turn):
    # Sempre mant√©m √∫ltimas 10 mensagens
    if (current_turn - msg.turn) <= 10:
        return True

    # Mant√©m mensagens marcadas como "important"
    if msg.metadata.get("important"):
        return True

    # Mant√©m se foi mencionada recentemente
    if msg.id in recent_references:
        return True

    # Caso contr√°rio, drop e summarize
    return False
```

**Exemplo CODEXA (Long Sessions)**:
```
User est√° em sess√£o de 2h usando Anuncio Agent, gerou 30 an√∫ncios.

Turn 1-10: Aprendendo prefer√™ncias do usu√°rio
Turn 11-20: Iterando em an√∫ncios espec√≠ficos
Turn 21-30: Refinamento final

Na Turn 25:
- Context: Turns 15-25 (√∫ltimas 10) + SUMMARY(Turns 1-14)
- Summary: "User prefer√™ncia: tom formal, foco B2B, keywords long-tail priority"
- Total tokens: 8k (messages) + 200 (summary) + 10k (knowledge) = 18k ‚úÖ
```

### M√©tricas para Monitorar Context Usage

**Dashboard essencial**:
```
Current context size: [X] / [LIMIT] tokens ([Y]% usage)

Breakdown:
- System prompt: 2,500 tokens (5%)
- Conversation history: 12,000 tokens (24%)
- Knowledge files: 15,000 tokens (30%)
- User input: 1,000 tokens (2%)
- Buffer for output: 19,500 tokens (39%)

Status: ‚úÖ HEALTHY (within limits with margin)
```

**Red flags**:
- üî¥ Context usage >90% (risco de truncation)
- üü° Context usage 70-90% (planejar compression)
- üü¢ Context usage <70% (healthy)

### Advanced Patterns

**Pattern: Context Caching** (quando dispon√≠vel)
```
Alguns LLMs (ex: Claude) permitem cachear partes do contexto:

CACHED (billed once):
- System prompt (2k tokens)
- Knowledge base files (15k tokens)

NOT CACHED (billed every call):
- Conversation history (8k tokens)
- User input (1k tokens)

Cost savings: ~40-60% em sess√µes longas
```

**Pattern: Dynamic Context Assembly**
```python
def assemble_context(user_query, session_history, max_tokens=50000):
    context_budget = max_tokens
    context_parts = []

    # 1. Always include: System prompt (highest priority)
    system_prompt = load_system_prompt()  # 2k tokens
    context_parts.append(system_prompt)
    context_budget -= len_tokens(system_prompt)

    # 2. Include: Relevant knowledge (semantic search)
    knowledge = semantic_search(user_query, top_k=3)  # ~15k tokens
    context_parts.append(knowledge)
    context_budget -= len_tokens(knowledge)

    # 3. Include: Recent history (sliding window)
    history_budget = context_budget * 0.4  # Reserve 40% for history
    recent_history = get_recent_messages(session_history, token_limit=history_budget)
    context_parts.append(recent_history)
    context_budget -= len_tokens(recent_history)

    # 4. Reserve: Output space (minimum 20% of total)
    output_space = max_tokens * 0.2
    if context_budget < output_space:
        # Trim history to make room for output
        recent_history = trim_to_fit(recent_history, context_budget - output_space)

    return "".join(context_parts)
```

### Troubleshooting Context Issues

**Sintoma**: "LLM parece "esquecer" informa√ß√µes que eu dei antes"
**Causa prov√°vel**: Context window excedido, mensagens antigas foram truncadas
**Fix**: Implement sliding window + summarization das partes antigas

**Sintoma**: "Outputs ficaram gen√©ricos, sem personalizac√£o"
**Causa prov√°vel**: Knowledge base n√£o foi carregado (selective loading falhou)
**Fix**: Debug semantic search, verificar se embeddings est√£o corretos

**Sintoma**: "Lat√™ncia aumentou 5x ap√≥s 20 mensagens"
**Causa prov√°vel**: Context crescendo linearmente sem compress√£o
**Fix**: Implement context rotation, limite hist√≥rico a √∫ltimas N messages

**Sintoma**: "LLM mistura informa√ß√£o de contextos diferentes"
**Causa prov√°vel**: Context pollution (informa√ß√£o n√£o-relevante no contexto)
**Fix**: More aggressive selective loading, clear context boundaries

### Best Practices CODEXA

1. **iso_vectorstore**: 20 arquivos, cada ~5-10k tokens
   - Total: ~100-200k tokens potencial
   - Selective load: 3-5 arquivos por query (~15-50k tokens real)

2. **Conversation history**: Sliding window de 10-15 mensagens
   - Summarize mensagens antigas (1-9) em ~200 tokens
   - Keep mensagens recentes (10-15) completas

3. **System prompts**: Compactos e modulares
   - Main prompt: ~2-3k tokens
   - Task-specific additions: +1-2k tokens
   - Total system: <5k tokens

4. **Output buffer**: Reserve 20-30% do context window
   - Se limit √© 200k tokens, use no m√°ximo 140k para input
   - Reserve 60k para output (evita truncation)

---

**Tags**: context-management, context-window, optimization, llm, compression, chunking
**Palavras-chave**: Context management, window limit, compression, chunking, selective loading
**Origem**: curso_agent/PRIME.md + mentor_agent/PRIME.md (discovery-first pattern)
**Processado**: 20251120
