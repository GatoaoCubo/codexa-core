# ğŸ¤– META-GUIA: INSTRUINDO LLMS A CRIAR DOCUMENTAÃ‡ÃƒO TÃ‰CNICA DE EXCELÃŠNCIA
## Como MÃ¡quinas Ensinam MÃ¡quinas: DestilaÃ§Ã£o de Conhecimento e Arquitetura de InformaÃ§Ã£o para IA

**VersÃ£o:** 1.0  
**Objetivo:** Guia definitivo para criar documentaÃ§Ã£o tÃ©cnica consumÃ­vel e eficiente para Large Language Models  
**Baseado em:** HuggingFace SmolLM Training Playbook, Alignment Handbook, e prÃ¡ticas de Knowledge Distillation  
**PÃºblico:** LLMs criando docs para LLMs, Desenvolvedores, AI Engineers, Technical Writers

---

## ğŸ“š ÃNDICE COMPLETO

1. [Fundamentos: Como LLMs Aprendem](#1-fundamentos)
2. [Arquitetura de Conhecimento para IA](#2-arquitetura-de-conhecimento)
3. [Formatos Ã“timos de DocumentaÃ§Ã£o](#3-formatos-Ã³timos)
4. [DestilaÃ§Ã£o de Conhecimento](#4-destilaÃ§Ã£o-de-conhecimento)
5. [Metodologias de Treinamento (SmolLM Approach)](#5-metodologias-de-treinamento)
6. [Supervised Fine-Tuning (SFT) para DocumentaÃ§Ã£o](#6-sft-para-documentaÃ§Ã£o)
7. [Preference Alignment e DPO](#7-preference-alignment)
8. [Estruturas de Prompt Engineering](#8-prompt-engineering)
9. [Dataset Curation e Data Quality](#9-dataset-curation)
10. [Evaluation Metrics para DocumentaÃ§Ã£o](#10-evaluation-metrics)
11. [PadrÃµes e Anti-PadrÃµes](#11-padrÃµes-e-anti-padrÃµes)
12. [Frameworks de ImplementaÃ§Ã£o](#12-frameworks)
13. [Casos de Uso e Templates](#13-casos-de-uso)
14. [ReferÃªncias e Bibliografia](#14-referÃªncias)

---

## 1. FUNDAMENTOS: COMO LLMS APRENDEM

### 1.1 O Pipeline de Aprendizado de um LLM

```
PRETRAINING â†’ SUPERVISED FINE-TUNING (SFT) â†’ PREFERENCE ALIGNMENT â†’ DEPLOYMENT
    â†“                    â†“                           â†“                    â†“
General Language    Task-Specific Skills      Human Alignment      Production Use
11T tokens          10k-100k examples         Preference Data      Real Users
```

**Insight CrÃ­tico:** DocumentaÃ§Ã£o tÃ©cnica Ã© consumida primariamente na fase de **context window** durante inferÃªncia, nÃ£o durante treinamento. Portanto:

- âœ… **Otimizar para retrieval**: Estrutura que facilita busca semÃ¢ntica
- âœ… **Densidade de informaÃ§Ã£o**: MÃ¡ximo conhecimento/token
- âœ… **Clareza estrutural**: Hierarquia explÃ­cita facilita parsing
- âœ… **RedundÃ¢ncia estratÃ©gica**: Conceitos-chave repetidos em diferentes contextos

### 1.2 Teoria de AtenÃ§Ã£o e Consumo de DocumentaÃ§Ã£o

**Mecanismo de AtenÃ§Ã£o:**
```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

Onde:
Q = Query (o que o modelo estÃ¡ procurando)
K = Key (Ã­ndices de conteÃºdo no documento)
V = Value (o conteÃºdo real)
```

**ImplicaÃ§Ãµes para DocumentaÃ§Ã£o:**

1. **Headers como Keys**: TÃ­tulos e subtÃ­tulos servem como "keys" que o modelo usa para navegar
2. **ConteÃºdo como Values**: O texto substantivo Ã© o "value" recuperado
3. **DistÃ¢ncia Importa**: InformaÃ§Ãµes relacionadas devem estar prÃ³ximas (window context)

**Exemplo de Estrutura Otimizada:**

```markdown
## Nome da FunÃ§Ã£o: calculate_similarity()

**O que faz:** Calcula similaridade coseno entre dois vetores
**Quando usar:** Comparar embeddings, busca semÃ¢ntica, clustering
**Input:** dois arrays numpy de mesma dimensÃ£o
**Output:** float entre -1 e 1 (1 = idÃªnticos)
**Complexidade:** O(n) onde n = dimensÃ£o dos vetores

### ImplementaÃ§Ã£o
```python
def calculate_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

### Exemplos
```python
# Vetores similares
v1 = [1, 2, 3]
v2 = [1, 2, 3.1]
similarity = calculate_similarity(v1, v2)  # ~0.999

# Vetores ortogonais
v3 = [1, 0]
v4 = [0, 1]
similarity = calculate_similarity(v3, v4)  # 0.0
```

### Edge Cases
- Vetores zero: retorna NaN (dividir por zero)
- DimensÃµes diferentes: erro ValueError
- SoluÃ§Ã£o: adicionar validaÃ§Ã£o de input
```

**Por que isso funciona melhor:**
- **ProgressÃ£o lÃ³gica**: O que â†’ Quando â†’ Como â†’ Exemplos â†’ Problemas
- **Scanning rÃ¡pido**: Headers permitem LLM "pular" para seÃ§Ã£o relevante
- **Contexto local**: Cada seÃ§Ã£o Ã© self-contained
- **RedundÃ¢ncia Ãºtil**: "Similaridade coseno" aparece 3x em diferentes formatos

### 1.3 Janela de Contexto e Arquitetura de InformaÃ§Ã£o

**LimitaÃ§Ãµes de Context Window:**

| Modelo | Context Window | Tokens/PÃ¡gina | PÃ¡ginas Doc |
|--------|----------------|---------------|-------------|
| GPT-4 | 128k tokens | ~500 | ~256 pÃ¡ginas |
| Claude Sonnet 4 | 200k tokens | ~500 | ~400 pÃ¡ginas |
| SmolLM2-1.7B | 8k tokens | ~500 | ~16 pÃ¡ginas |

**EstratÃ©gias por Tamanho de Janela:**

**Para modelos pequenos (< 8k tokens):**
```markdown
# EstratÃ©gia: Chunking + Cross-referencing

## MÃ³dulo 1: Core Concepts [FILE: 01_core.md]
[ConteÃºdo essencial, ~2k tokens]

## MÃ³dulo 2: Advanced Topics [FILE: 02_advanced.md]
Prerequisite: Leia 01_core.md primeiro
[ConteÃºdo avanÃ§ado, ~2k tokens]
```

**Para modelos grandes (> 100k tokens):**
```markdown
# EstratÃ©gia: Documento monolÃ­tico com Ã­ndice

## ÃNDICE NAVEGÃVEL
- [1. Conceitos](#conceitos)
- [2. API Reference](#api)
- [3. Exemplos](#exemplos)
[... 50 pÃ¡ginas de conteÃºdo detalhado ...]
```

### 1.4 TokenizaÃ§Ã£o e Densidade SemÃ¢ntica

**Como Tokenizers Afetam DocumentaÃ§Ã£o:**

```python
# Exemplo: Mesmo conceito, diferentes tokenizaÃ§Ãµes

# OpÃ§Ã£o A (verbose, mais tokens)
"This function calculates the similarity between two vectors"
# GPT-4 tokenizer: 11 tokens

# OpÃ§Ã£o B (concisa, menos tokens)
"Calculates vector similarity"
# GPT-4 tokenizer: 4 tokens

# OpÃ§Ã£o C (otimizada)
"calculate_similarity(v1, v2) â†’ cosine similarity"
# GPT-4 tokenizer: 8 tokens, mas MUITO mais informaÃ§Ã£o
```

**PrincÃ­pio de Densidade SemÃ¢ntica:**
```
Densidade = InformaÃ§Ã£o_Ãºtil / Total_tokens

Objetivo: Maximizar densidade mantendo clareza
```

**TÃ©cnicas de MaximizaÃ§Ã£o:**

1. **Code over prose quando possÃ­vel**
```markdown
âŒ Menos denso:
"Para iterar sobre uma lista em Python, vocÃª usa um loop for. 
A sintaxe Ã©: for item in lista, onde item Ã© cada elemento."

âœ… Mais denso:
```python
# Iterar lista
for item in lista:
    process(item)
```
```

2. **Tabelas para comparaÃ§Ãµes**
```markdown
âŒ Menos denso:
"O mÃ©todo GET Ã© usado para recuperar dados e Ã© idempotente. 
O mÃ©todo POST Ã© usado para criar recursos e nÃ£o Ã© idempotente..."

âœ… Mais denso:
| MÃ©todo | Uso | Idempotente | Body |
|--------|-----|-------------|------|
| GET | Retrieve | âœ“ | âœ— |
| POST | Create | âœ— | âœ“ |
| PUT | Update | âœ“ | âœ“ |
| DELETE | Remove | âœ“ | âœ— |
```

3. **Exemplos inline**
```markdown
âŒ Separado (mais tokens para mesma info):
**FunÃ§Ã£o:** add()
**DescriÃ§Ã£o:** Soma dois nÃºmeros
**Exemplo:** Ver seÃ§Ã£o 5.3

âœ… Integrado (mais denso):
`add(a, b)` â†’ `a + b` | Ex: `add(2,3)=5`
```

---

## 2. ARQUITETURA DE CONHECIMENTO PARA IA

### 2.1 Hierarquia de AbstraÃ§Ã£o

**PirÃ¢mide de Conhecimento:**

```
                    [CONCEITOS]
                   /           \
              [PATTERNS]    [PRINCIPLES]
             /        \        /        \
       [APIS]      [EXAMPLES]      [TUTORIALS]
      /      \      /      \       /         \
[REFERENCE] [CODE] [TESTS] [DEMOS] [QUICKSTART]
```

**Cada nÃ­vel serve um propÃ³sito:**

1. **CONCEITOS** (topo): AbstraÃ§Ãµes fundamentais, invariantes
   - "Um transformer usa self-attention para processar sequÃªncias"
   - Muda raramente, alta reutilizaÃ§Ã£o

2. **PATTERNS**: SoluÃ§Ãµes recorrentes
   - "Para fine-tuning eficiente, use LoRA"
   - AplicÃ¡vel em mÃºltiplos contextos

3. **APIS/EXAMPLES**: ImplementaÃ§Ãµes concretas
   - `trainer.train()` cÃ³digo especÃ­fico
   - Muda frequentemente, baixa abstraÃ§Ã£o

**PrincÃ­pio de OrganizaÃ§Ã£o:**

```python
class DocumentStructure:
    """
    Organize do abstrato ao concreto,
    do conceitual ao operacional
    """
    
    def __init__(self):
        self.layers = {
            'why': 'Conceitos e motivaÃ§Ã£o',
            'what': 'DefiniÃ§Ãµes e componentes',
            'how': 'ImplementaÃ§Ã£o e uso',
            'examples': 'Casos concretos',
            'reference': 'Detalhes exaustivos'
        }
    
    def generate_doc(self):
        """
        Gera documentaÃ§Ã£o seguindo hierarquia
        """
        doc = []
        
        # Layer 1: Why (Contexto)
        doc.append("## Por que este mÃ³dulo existe?")
        doc.append(self.explain_motivation())
        
        # Layer 2: What (DefiniÃ§Ãµes)
        doc.append("## O que este mÃ³dulo faz?")
        doc.append(self.define_components())
        
        # Layer 3: How (Uso)
        doc.append("## Como usar?")
        doc.append(self.show_basic_usage())
        
        # Layer 4: Examples (Concreto)
        doc.append("## Exemplos prÃ¡ticos")
        doc.append(self.provide_examples())
        
        # Layer 5: Reference (Completo)
        doc.append("## API Reference")
        doc.append(self.full_api_reference())
        
        return "\n\n".join(doc)
```

### 2.2 Grafo de Conhecimento vs. Estrutura Linear

**Problema:** DocumentaÃ§Ã£o linear nÃ£o captura relaÃ§Ãµes complexas

**SoluÃ§Ã£o:** Embedar grafo de conhecimento em estrutura linear

**Exemplo de ImplementaÃ§Ã£o:**

```markdown
# Sistema de Treinamento de LLM

## Componentes Principais

### 1. Dataset [ID: dataset]
- **DependÃªncias:** Nenhuma
- **Usado por:** [DataLoader](#dataloader), [Tokenizer](#tokenizer)
- **Relacionado:** [Data Curation](#data-curation)

```python
class Dataset:
    def __init__(self, data_path):
        self.data = load_data(data_path)
```

### 2. DataLoader [ID: dataloader]
- **DependÃªncias:** [Dataset](#dataset)
- **Usado por:** [Trainer](#trainer)
- **Relacionado:** [Batching Strategy](#batching)

```python
class DataLoader:
    def __init__(self, dataset, batch_size):
        self.dataset = dataset  # â† ReferÃªncia explÃ­cita
        self.batch_size = batch_size
```

### 3. Trainer [ID: trainer]
- **DependÃªncias:** [DataLoader](#dataloader), [Model](#model), [Optimizer](#optimizer)
- **Usado por:** [Training Pipeline](#pipeline)
- **Workflow:** 
  1. Recebe [DataLoader](#dataloader)
  2. Itera batches
  3. Calcula loss com [Model](#model)
  4. Atualiza pesos com [Optimizer](#optimizer)

```python
class Trainer:
    def __init__(self, model, dataloader, optimizer):
        self.model = model
        self.dataloader = dataloader
        self.optimizer = optimizer
    
    def train(self):
        for batch in self.dataloader:  # â† Usa DataLoader
            loss = self.model(batch)    # â† Usa Model
            self.optimizer.step()        # â† Usa Optimizer
```

## Grafo de DependÃªncias Visual

```
Dataset â†’ DataLoader â†’ Trainer â†’ Output
   â†“          â†“          â†‘
Tokenizer    â†“       Model
             â†“          â†‘
         BatchSampler  Optimizer
```
```

**Vantagens desta abordagem:**

1. **NavegaÃ§Ã£o nÃ£o-linear**: LLM pode seguir links [ID](#id)
2. **Contexto explÃ­cito**: "Usado por" e "DependÃªncias" sÃ£o declarados
3. **RedundÃ¢ncia Ãºtil**: InformaÃ§Ã£o repetida em contextos diferentes
4. **Visual + Textual**: Diagrama + prosa para mÃºltiplos estilos de aprendizado

### 2.3 PadrÃ£o de Conhecimento Progressivo

**Progressive Disclosure Pattern:**

```markdown
# FunÃ§Ã£o: train_model()

## TL;DR (10 segundos)
```python
trainer.train_model(data, epochs=3)  # Treina modelo por 3 Ã©pocas
```

## Quick Start (2 minutos)
```python
from trainer import Trainer

# 1. Preparar dados
data = load_dataset("my_dataset")

# 2. Criar trainer
trainer = Trainer(model="gpt2")

# 3. Treinar
results = trainer.train_model(
    data=data,
    epochs=3,
    learning_rate=1e-4
)

print(f"Loss final: {results.final_loss}")
```

## Guia Completo (10 minutos)

### ParÃ¢metros Detalhados

**epochs** (int, default=3)
- NÃºmero de passadas completas pelo dataset
- Mais Ã©pocas = mais aprendizado, risco de overfitting
- Recomendado: 3-5 para fine-tuning, 1 para grandes datasets

**learning_rate** (float, default=1e-4)
- Taxa de atualizaÃ§Ã£o dos pesos
- Muito alto: instabilidade
- Muito baixo: convergÃªncia lenta
- Recomendado: 1e-4 a 1e-5 para fine-tuning

[... 50 parÃ¡grafos de detalhes ...]

## API Reference (Completo)
[... documentaÃ§Ã£o exaustiva ...]
```

**Por que funciona:**

- **Respeita nÃ­veis de expertise**: UsuÃ¡rio novato lÃª TL;DR, expert pula para API Reference
- **FÃ¡cil scanning**: LLM pode identificar qual seÃ§Ã£o precisa
- **NÃ£o polui context**: Iniciante nÃ£o Ã© forÃ§ado a consumir 50 parÃ¡grafos

### 2.4 Taxonomia e CategorizaÃ§Ã£o

**Sistema de Tags SemÃ¢ntico:**

```markdown
# FunÃ§Ã£o: calculate_loss()

**Categoria:** Training / Loss Functions  
**Complexidade:** â­â­â­ (IntermediÃ¡rio)  
**Tipo:** Utility Function  
**DomÃ­nio:** Deep Learning > Optimization  
**Keywords:** loss, training, backpropagation, gradient  
**Related:** [optimizer.step()](#optimizer), [backward()](#backward)  
**Version Added:** 1.0  
**Stability:** Stable  

---

[DocumentaÃ§Ã£o da funÃ§Ã£o...]
```

**BenefÃ­cios para LLM:**

1. **ClassificaÃ§Ã£o multi-dimensional**: LLM pode filtrar por categoria, complexidade, domÃ­nio
2. **Keywords explÃ­citas**: Melhoram retrieval semÃ¢ntico
3. **RelaÃ§Ãµes explicitadas**: "Related" cria grafo de conhecimento
4. **Metadados Ãºteis**: Version/Stability informam confiabilidade

**Sistema de CategorizaÃ§Ã£o HierÃ¡rquico:**

```
Sistema de IA
â”œâ”€â”€ Pretraining
â”‚   â”œâ”€â”€ Datasets
â”‚   â”‚   â”œâ”€â”€ Web Data (FineWeb, DCLM)
â”‚   â”‚   â”œâ”€â”€ Code Data (StarCoder)
â”‚   â”‚   â””â”€â”€ Math Data (FineMath)
â”‚   â”œâ”€â”€ Architecture
â”‚   â”‚   â””â”€â”€ Transformer
â”‚   â””â”€â”€ Training
â”‚       â”œâ”€â”€ Distributed Training
â”‚       â””â”€â”€ Mixed Precision
â”œâ”€â”€ Fine-tuning
â”‚   â”œâ”€â”€ Supervised Fine-Tuning (SFT)
â”‚   â”‚   â”œâ”€â”€ SFTTrainer
â”‚   â”‚   â””â”€â”€ Dataset Formatting
â”‚   â””â”€â”€ Preference Alignment
â”‚       â”œâ”€â”€ DPO (Direct Preference Optimization)
â”‚       â”œâ”€â”€ RLHF (Reinforcement Learning from Human Feedback)
â”‚       â””â”€â”€ IPO (Identity Preference Optimization)
â””â”€â”€ Deployment
    â”œâ”€â”€ Quantization
    â”œâ”€â”€ Inference Optimization
    â””â”€â”€ Model Serving
```

**Como embedar taxonomia em docs:**

```markdown
# SFTTrainer

**Caminho na Taxonomia:**  
`Sistema de IA > Fine-tuning > Supervised Fine-Tuning (SFT) > SFTTrainer`

**Conceito Pai:** [Supervised Fine-Tuning](#sft)  
**Conceitos IrmÃ£os:** [DatasetFormatting](#dataset-formatting)  
**Conceitos Filhos:** [TrainingArguments](#training-arguments), [Callbacks](#callbacks)

---

[DocumentaÃ§Ã£o...]
```

---

## 3. FORMATOS Ã“TIMOS DE DOCUMENTAÃ‡ÃƒO

### 3.1 Markdown Estruturado

**Por que Markdown:**
- âœ… Human-readable E machine-parseable
- âœ… Hierarquia clara via headers (#, ##, ###)
- âœ… Code blocks nativos
- âœ… Tables, lists, links built-in
- âœ… Git-friendly (diffs legÃ­veis)

**Template de DocumentaÃ§Ã£o Ã“tima:**

```markdown
# [NOME DO COMPONENTE]

> **TL;DR:** [Uma linha descrevendo o que faz]

## Metadados
- **Tipo:** [Class|Function|Module|Concept]
- **Complexidade:** â­â­â­â˜†â˜†
- **Estabilidade:** [Stable|Beta|Experimental]
- **VersÃ£o:** 2.0

## VisÃ£o Geral

[2-3 parÃ¡grafos explicando:]
- O que Ã©
- Por que existe (problema que resolve)
- Quando usar vs. alternativas

## Quick Start

```python
# Exemplo mÃ­nimo funcional
resultado = componente.usar(input)
```

## Conceitos-Chave

### Conceito 1: [Nome]
[ExplicaÃ§Ã£o clara]

### Conceito 2: [Nome]
[ExplicaÃ§Ã£o clara]

## Guia de Uso

### Caso de Uso 1: [Nome]

**CenÃ¡rio:** [Quando usar]

```python
# CÃ³digo completo comentado
componente = Componente(config)
result = componente.process(data)
```

**Resultado esperado:** [SaÃ­da]

### Caso de Uso 2: [Nome]
[Estrutura similar]

## API Reference

### Classe/FunÃ§Ã£o: nome()

**Assinatura:**
```python
def nome(
    param1: Type,
    param2: Type = default
) -> ReturnType:
```

**ParÃ¢metros:**
- `param1` (Type): [DescriÃ§Ã£o clara, range/constraints]
- `param2` (Type, default=default): [DescriÃ§Ã£o]

**Retorna:**
- Type: [O que retorna, formato]

**Raises:**
- `ErrorType`: [Quando ocorre]

**Exemplos:**

```python
# Exemplo 1: Uso bÃ¡sico
result = nome(arg1, arg2)

# Exemplo 2: Com todos parÃ¢metros
result = nome(
    param1=valor,
    param2=outro_valor
)

# Exemplo 3: Edge case
try:
    result = nome(None)  # Erro esperado
except ValueError as e:
    print(f"Capturado: {e}")
```

## Troubleshooting

### Problema: [Erro comum]
**Sintoma:** [Como aparece]  
**Causa:** [Por que acontece]  
**SoluÃ§Ã£o:** [Como resolver]

```python
# CÃ³digo que resolve
solucao()
```

## Performance Considerations

- **Complexidade:** O(n log n)
- **MemÃ³ria:** O(n)
- **OtimizaÃ§Ãµes:** [Dicas especÃ­ficas]

## Related

- **Prerequisitos:** [Links para conceitos necessÃ¡rios]
- **PrÃ³ximos passos:** [Links para tÃ³picos avanÃ§ados]
- **Alternativas:** [Outras abordagens]

## Changelog

- **v2.0 (2025-01):** [MudanÃ§as breaking]
- **v1.5 (2024-12):** [Novos recursos]
- **v1.0 (2024-06):** [VersÃ£o inicial]

## References

- [Paper relevante](link)
- [Tutorial externo](link)
- [CÃ³digo-fonte](github-link)
```

### 3.2 JSON Schema para Structured Data

**Quando usar JSON em vez de Markdown:**
- âœ… Dados estruturados precisos (APIs, configs)
- âœ… ValidaÃ§Ã£o automÃ¡tica necessÃ¡ria
- âœ… Parsing programÃ¡tico frequente
- âœ… Schema evolution tracking

**Exemplo: DocumentaÃ§Ã£o de API como JSON Schema**

```json
{
  "api": "SFTTrainer",
  "version": "2.0",
  "type": "class",
  "description": "Trainer para Supervised Fine-Tuning de LLMs",
  
  "constructor": {
    "signature": "SFTTrainer(model, args, train_dataset, eval_dataset, tokenizer)",
    "parameters": [
      {
        "name": "model",
        "type": "PreTrainedModel",
        "required": true,
        "description": "Modelo base a ser fine-tunado",
        "example": "AutoModelForCausalLM.from_pretrained('gpt2')"
      },
      {
        "name": "args",
        "type": "TrainingArguments",
        "required": true,
        "description": "ConfiguraÃ§Ãµes de treinamento",
        "default_example": {
          "output_dir": "./results",
          "num_train_epochs": 3,
          "per_device_train_batch_size": 4,
          "learning_rate": 2e-5
        },
        "reference": "#TrainingArguments"
      }
    ]
  },
  
  "methods": [
    {
      "name": "train",
      "signature": "train() -> TrainOutput",
      "description": "Executa loop de treinamento",
      "returns": {
        "type": "TrainOutput",
        "fields": {
          "global_step": "int - NÃºmero total de steps",
          "training_loss": "float - Loss mÃ©dio final",
          "metrics": "dict - MÃ©tricas adicionais"
        }
      },
      "example": "output = trainer.train()",
      "complexity": {
        "time": "O(epochs * dataset_size / batch_size)",
        "space": "O(model_size + batch_size * seq_length)"
      }
    }
  ],
  
  "usage_patterns": [
    {
      "name": "Basic Training",
      "scenario": "Fine-tune modelo em dataset simples",
      "code": "trainer = SFTTrainer(model, args, dataset)\ntrainer.train()",
      "when_to_use": "Dataset pequeno (<10k samples), Ãºnica GPU"
    },
    {
      "name": "Distributed Training",
      "scenario": "Treinar em mÃºltiplas GPUs",
      "code": "# Usar com accelerate launch\ntrainer = SFTTrainer(...)\ntrainer.train()",
      "when_to_use": "Dataset grande, mÃºltiplas GPUs disponÃ­veis"
    }
  ],
  
  "related_components": [
    {"name": "TrainingArguments", "type": "config", "relationship": "dependency"},
    {"name": "DPOTrainer", "type": "class", "relationship": "alternative"},
    {"name": "Trainer", "type": "class", "relationship": "parent"}
  ]
}
```

**Vantagens para LLMs:**

1. **Parsing determinÃ­stico**: JSON Ã© nÃ£o-ambÃ­guo
2. **Schema validation**: Garante estrutura consistente
3. **Queryability**: FÃ¡cil extrair campos especÃ­ficos
4. **Composability**: JSONs podem referenciar outros via IDs

### 3.3 CÃ³digo Auto-Documentado

**PrincÃ­pio:** CÃ³digo bem escrito Ã‰ documentaÃ§Ã£o

**Exemplo de CÃ³digo que Documenta a Si Mesmo:**

```python
from dataclasses import dataclass
from typing import Optional, List
from enum import Enum

class OptimizerType(Enum):
    """Tipos de otimizadores suportados"""
    ADAM = "adam"
    ADAMW = "adamw"
    SGD = "sgd"

@dataclass
class TrainingConfig:
    """
    ConfiguraÃ§Ã£o para treinamento de LLM.
    
    Attributes:
        learning_rate: Taxa de aprendizado inicial. Valores tÃ­picos: 1e-5 a 1e-4
        batch_size: Tamanho do batch por dispositivo. Maior = mais rÃ¡pido mas mais memÃ³ria
        num_epochs: NÃºmero de Ã©pocas de treinamento. TÃ­pico: 3-5 para fine-tuning
        optimizer: Tipo de otimizador. AdamW recomendado para a maioria dos casos
        warmup_steps: Steps de warmup linear. Recomendado: 10% do total de steps
        
    Example:
        >>> config = TrainingConfig(
        ...     learning_rate=2e-5,
        ...     batch_size=8,
        ...     num_epochs=3
        ... )
    """
    learning_rate: float = 2e-5
    batch_size: int = 8
    num_epochs: int = 3
    optimizer: OptimizerType = OptimizerType.ADAMW
    warmup_steps: Optional[int] = None
    
    def __post_init__(self):
        """ValidaÃ§Ã£o e ajustes automÃ¡ticos"""
        if self.warmup_steps is None:
            # HeurÃ­stica: 10% de warmup Ã© um bom padrÃ£o
            total_steps = self.estimate_total_steps()
            self.warmup_steps = int(0.1 * total_steps)
    
    def estimate_total_steps(self) -> int:
        """
        Estima nÃºmero total de steps de treinamento.
        
        Returns:
            int: NÃºmero estimado de gradient updates
            
        Note:
            Assume dataset de 10k samples. Para dataset real,
            calcular como: len(dataset) * num_epochs / batch_size
        """
        ASSUMED_DATASET_SIZE = 10_000
        return (ASSUMED_DATASET_SIZE * self.num_epochs) // self.batch_size


class Trainer:
    """
    Trainer para Supervised Fine-Tuning de LLMs.
    
    Este trainer implementa o loop de treinamento padrÃ£o:
    1. Carrega batch de dados
    2. Forward pass (calcula loss)
    3. Backward pass (calcula gradientes)
    4. Optimizer step (atualiza pesos)
    5. Repete atÃ© completar Ã©pocas
    
    Example:
        >>> from transformers import AutoModel
        >>> model = AutoModel.from_pretrained("gpt2")
        >>> config = TrainingConfig(num_epochs=3)
        >>> trainer = Trainer(model, config)
        >>> trainer.train(dataset)
        TrainOutput(loss=0.5, perplexity=1.6)
    
    See Also:
        - TrainingConfig: Para configurar hiperparÃ¢metros
        - Dataset: Para formatar dados de treinamento
    """
    
    def __init__(self, model, config: TrainingConfig):
        self.model = model
        self.config = config
        self._setup_optimizer()
    
    def _setup_optimizer(self):
        """
        Inicializa otimizador baseado em config.
        
        Private method - usuÃ¡rios nÃ£o devem chamar diretamente.
        """
        if self.config.optimizer == OptimizerType.ADAMW:
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate
            )
        # ... outros otimizadores
    
    def train(self, dataset: "Dataset") -> "TrainOutput":
        """
        Executa loop de treinamento completo.
        
        Args:
            dataset: Dataset formatado com campos 'input_ids' e 'labels'
        
        Returns:
            TrainOutput com mÃ©tricas finais (loss, perplexity, etc)
        
        Raises:
            ValueError: Se dataset estÃ¡ vazio ou mal formatado
            RuntimeError: Se GPU fica sem memÃ³ria
        
        Example:
            >>> dataset = load_dataset("my_data")
            >>> output = trainer.train(dataset)
            >>> print(f"Loss final: {output.loss:.3f}")
            Loss final: 0.523
        
        Note:
            Este mÃ©todo modifica o modelo in-place. Se quiser manter
            modelo original, faÃ§a uma cÃ³pia antes:
            
            >>> import copy
            >>> original_model = copy.deepcopy(model)
            >>> trainer.train(dataset)  # model Ã© modificado
            >>> # original_model permanece intacto
        """
        if len(dataset) == 0:
            raise ValueError("Dataset nÃ£o pode estar vazio")
        
        total_loss = 0.0
        num_steps = 0
        
        for epoch in range(self.config.num_epochs):
            for batch in dataset:
                # Forward pass
                loss = self._compute_loss(batch)
                
                # Backward pass
                loss.backward()
                
                # Optimizer step
                self.optimizer.step()
                self.optimizer.zero_grad()
                
                total_loss += loss.item()
                num_steps += 1
        
        avg_loss = total_loss / num_steps
        return TrainOutput(loss=avg_loss, perplexity=math.exp(avg_loss))
    
    def _compute_loss(self, batch) -> torch.Tensor:
        """
        Calcula loss para um batch.
        
        ImplementaÃ§Ã£o: Cross-entropy loss padrÃ£o.
        Para LLMs causal, isso Ã© equivalente a next-token prediction.
        """
        outputs = self.model(**batch)
        return outputs.loss
```

**Por que este cÃ³digo Ã© excelente documentaÃ§Ã£o:**

1. **Type hints**: LLM sabe exatamente tipos esperados
2. **Docstrings estruturadas**: Segue formato padrÃ£o (Google/NumPy style)
3. **Exemplos inline**: Cada classe/mÃ©todo tem exemplo de uso
4. **Enums explÃ­citos**: OptimizerType documenta opÃ§Ãµes vÃ¡lidas
5. **ValidaÃ§Ã£o clara**: __post_init__ mostra o que Ã© validado
6. **ComentÃ¡rios estratÃ©gicos**: Apenas onde adiciona valor
7. **Nomes descritivos**: `_setup_optimizer` Ã© auto-explicativo
8. **Notas Ãºteis**: "Note:" adiciona contexto importante

### 3.4 Diagramas e VisualizaÃ§Ãµes em Texto

**Mermaid para Fluxogramas:**

```markdown
## Fluxo de Treinamento

```mermaid
graph TD
    A[Dataset Raw] -->|Tokenize| B[Tokenized Dataset]
    B -->|DataLoader| C[Batches]
    C -->|Forward Pass| D[Model Output]
    D -->|Calculate Loss| E[Loss Value]
    E -->|Backward Pass| F[Gradients]
    F -->|Optimizer Step| G[Updated Weights]
    G -->|Next Batch| C
    C -->|All Batches Done| H[Epoch Complete]
    H -->|More Epochs?| C
    H -->|Training Done| I[Trained Model]
    
    style A fill:#e1f5ff
    style I fill:#c8e6c9
```
```

**ASCII Art para Arquiteturas:**

```markdown
## Arquitetura do Transformer

```
Input Embedding (512-dim)
        â†“
   Positional Encoding
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Encoder Layer  â”‚ x N (e.g. 12)
   â”‚                â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚Multi-Head  â”‚ â”‚
   â”‚ â”‚ Attention  â”‚ â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â”‚       â†“        â”‚
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
   â”‚ â”‚Feed Forwardâ”‚ â”‚
   â”‚ â”‚  Network   â”‚ â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   Output (logits)
        â†“
    Softmax
        â†“
   Probabilities
```
```

**Tabelas para ComparaÃ§Ãµes:**

```markdown
## ComparaÃ§Ã£o de MÃ©todos de Fine-Tuning

| MÃ©todo | Params TreinÃ¡veis | MemÃ³ria GPU | Tempo | Qualidade | Quando Usar |
|--------|-------------------|-------------|-------|-----------|-------------|
| Full Fine-Tuning | 100% | Alta (ex: 80GB) | Lento | â­â­â­â­â­ | Dataset grande, recurso ilimitado |
| LoRA | ~0.1% | Baixa (ex: 16GB) | RÃ¡pido | â­â­â­â­ | Maioria dos casos, eficiÃªncia |
| Prefix Tuning | ~0.01% | Muito Baixa | Muito RÃ¡pido | â­â­â­ | Hardware limitado, prototipagem |
| Adapter Tuning | ~1% | MÃ©dia | MÃ©dio | â­â­â­â­ | MÃºltiplas tarefas, modularidade |

**RecomendaÃ§Ã£o:**
- Dataset < 10k samples: LoRA
- Dataset > 100k samples + GPU potente: Full Fine-Tuning
- Prototipagem rÃ¡pida: Prefix Tuning
```

---

## 4. DESTILAÃ‡ÃƒO DE CONHECIMENTO

### 4.1 O que Ã© Knowledge Distillation

**DefiniÃ§Ã£o:**
Processo de transferir conhecimento de um modelo "professor" (grande, complexo) para um modelo "aluno" (pequeno, eficiente).

**Aplicado a DocumentaÃ§Ã£o:**
Transformar conhecimento complexo em formato digestÃ­vel sem perder essÃªncia.

**Paralelo: LLM Grande â†’ LLM Pequeno = Doc Complexa â†’ Doc AcessÃ­vel**

```python
class KnowledgeDistillation:
    """
    Framework conceitual para destilar conhecimento em docs
    """
    
    def distill(self, complex_knowledge):
        """
        Processo de destilaÃ§Ã£o em 4 etapas
        """
        # 1. EXTRACT: Identificar conceitos-chave
        key_concepts = self.extract_core_concepts(complex_knowledge)
        
        # 2. SIMPLIFY: Reduzir complexidade sem perder precisÃ£o
        simplified = self.simplify_concepts(key_concepts)
        
        # 3. EXEMPLIFY: Adicionar exemplos concretos
        with_examples = self.add_examples(simplified)
        
        # 4. VALIDATE: Testar compreensÃ£o
        validated = self.validate_understanding(with_examples)
        
        return validated
```

### 4.2 TÃ©cnicas de DestilaÃ§Ã£o para DocumentaÃ§Ã£o

#### TÃ©cnica 1: Abstraction Ladder (Escada de AbstraÃ§Ã£o)

**PrincÃ­pio:** Apresentar mesmo conceito em mÃºltiplos nÃ­veis de abstraÃ§Ã£o

**Exemplo: Explicando "Attention Mechanism"**

```markdown
## Attention Mechanism

### NÃ­vel 1: MetÃ¡fora (Mais abstrato)
Imagine que vocÃª estÃ¡ em uma festa barulhenta tentando ouvir um amigo.
VocÃª "presta atenÃ§Ã£o" na voz dele e ignora o ruÃ­do de fundo.
Attention em IA funciona similar: o modelo foca nas partes relevantes do input.

### NÃ­vel 2: Conceitual
Attention Ã© um mecanismo que permite o modelo:
- Calcular importÃ¢ncia relativa de diferentes partes do input
- Dar mais "peso" Ã s partes importantes
- Criar representaÃ§Ãµes contextualizadas

### NÃ­vel 3: MatemÃ¡tico
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V

Onde:
- Q (Query): O que estamos procurando
- K (Key): Ãndices do conteÃºdo
- V (Value): O conteÃºdo real
- Softmax: Normaliza scores em distribuiÃ§Ã£o de probabilidade
```

### NÃ­vel 4: ImplementaÃ§Ã£o
```python
def attention(Q, K, V):
    # Passo 1: Similaridade entre query e keys
    scores = torch.matmul(Q, K.transpose(-2, -1))
    
    # Passo 2: Escala por raiz de dimensÃ£o
    d_k = Q.size(-1)
    scaled_scores = scores / math.sqrt(d_k)
    
    # Passo 3: Softmax para probabilidades
    attention_weights = F.softmax(scaled_scores, dim=-1)
    
    # Passo 4: Weighted sum dos values
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

### NÃ­vel 5: Exemplo Concreto
```python
# Input: Frase "The cat sat on the mat"
# Query: representaÃ§Ã£o de "cat"
# Keys/Values: representaÃ§Ãµes de todas palavras

# Attention vai calcular:
# "cat" presta mais atenÃ§Ã£o em "sat" (verbo relacionado)
# "cat" presta menos atenÃ§Ã£o em "the" (palavra funÃ§Ã£o)

tokens = ["The", "cat", "sat", "on", "the", "mat"]
attention_weights_for_cat = [0.05, 0.30, 0.40, 0.10, 0.05, 0.10]
#                             â†‘     â†‘     â†‘â†‘
#                           baixo mÃ©dio alto â†’ "sat" Ã© mais importante
```
```

**Por que funciona:**
- **MÃºltiplos pontos de entrada**: Leitor escolhe nÃ­vel que faz sentido
- **Progressivo**: Cada nÃ­vel constrÃ³i sobre anterior
- **RedundÃ¢ncia inteligente**: Mesmo conceito, Ã¢ngulos diferentes

#### TÃ©cnica 2: Chunking Conceitual

**PrincÃ­pio:** Quebrar conceitos complexos em "chunks" independentes

**Exemplo: Explicando "Training Pipeline"**

```markdown
# Training Pipeline Completo

## VisÃ£o Geral (1 chunk - autossuficiente)
Pipeline de treinamento tem 4 fases:
1. **Data Preparation**: Carregar e processar dados
2. **Training Loop**: Iterar e atualizar modelo
3. **Evaluation**: Medir performance
4. **Checkpointing**: Salvar progresso

Cada fase Ã© independente e pode ser executada separadamente.

---

## FASE 1: Data Preparation [Chunk Independente]

**Objetivo:** Transformar dados raw em formato consumÃ­vel pelo modelo

**Input:** Dados raw (ex: texto, imagens)  
**Output:** DataLoader com batches processados

### Sub-etapas:
1. **Load**: Ler dados do disco/API
2. **Tokenize**: Converter texto em IDs
3. **Batch**: Agrupar em batches fixos
4. **Shuffle**: Randomizar ordem (evita overfitting)

### CÃ³digo:
```python
# Passo 1: Load
dataset = load_dataset("data.json")

# Passo 2: Tokenize
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenized = dataset.map(lambda x: tokenizer(x['text']))

# Passo 3: Batch + Shuffle
dataloader = DataLoader(
    tokenized,
    batch_size=8,
    shuffle=True
)
```

### Checkpoint de CompreensÃ£o:
- âœ“ VocÃª consegue explicar por que shuffle Ã© importante?
- âœ“ O que acontece se batch_size for muito grande?

**PrÃ³xima Fase:** [Training Loop](#fase-2)

---

## FASE 2: Training Loop [Chunk Independente]

**Prerequisito:** Completou [Data Preparation](#fase-1)

**Objetivo:** Atualizar pesos do modelo para minimizar loss

**Input:** DataLoader, Model, Optimizer  
**Output:** Model com pesos atualizados

### Sub-etapas:
1. **Forward Pass**: Calcular prediÃ§Ã£o e loss
2. **Backward Pass**: Calcular gradientes
3. **Optimizer Step**: Atualizar pesos
4. **Repeat**: Para todos batches e Ã©pocas

### CÃ³digo:
```python
for epoch in range(num_epochs):
    for batch in dataloader:  # De FASE 1
        # Forward
        outputs = model(**batch)
        loss = outputs.loss
        
        # Backward
        loss.backward()
        
        # Update
        optimizer.step()
        optimizer.zero_grad()
```

### Checkpoint de CompreensÃ£o:
- âœ“ Por que zero_grad() Ã© necessÃ¡rio?
- âœ“ O que acontece se esquecer backward()?

**PrÃ³xima Fase:** [Evaluation](#fase-3)

---

[... FASE 3 e FASE 4 seguem mesmo padrÃ£o ...]
```

**Vantagens:**
- **Modularidade**: Cada chunk Ã© autossuficiente
- **NÃ£o-linearidade**: Leitor pode pular chunks jÃ¡ conhecidos
- **Checkpoints**: ValidaÃ§Ã£o de compreensÃ£o por chunk

#### TÃ©cnica 3: Concept Maps (Mapas Conceituais)

**PrincÃ­pio:** Visualizar relaÃ§Ãµes entre conceitos

```markdown
## Mapa Conceitual: Fine-Tuning de LLMs

```
                   [Fine-Tuning]
                   /     |     \
                  /      |      \
                 /       |       \
          [SFT]     [RLHF]    [DPO]
            |          |         |
            â†“          â†“         â†“
      [Dataset    [Reward   [Preference
       Instruct]   Model]    Pairs]
            |          |         |
            â†“          â†“         â†“
      [Cross-    [PPO       [Direct
       Entropy    Training]  Optimization]
       Loss]

LEGENDA:
â†’ : "usa" ou "depende de"
âˆ¥ : "Ã© um tipo de"
âŠ• : "combina com"

RELAÃ‡Ã•ES IMPORTANTES:
- SFT Ã© prerequisito para RLHF e DPO
- RLHF Ã© mais complexo que DPO
- DPO nÃ£o precisa de reward model (mais simples)
```

### Exemplo Detalhado de Cada:

#### SFT (Supervised Fine-Tuning)
- **Input:** Pares (instruÃ§Ã£o, resposta ideal)
- **MÃ©todo:** Supervised learning padrÃ£o
- **Loss:** Cross-entropy
- **Quando usar:** Sempre - Ã© o primeiro passo

#### RLHF (Reinforcement Learning from Human Feedback)
- **Input:** ClassificaÃ§Ãµes humanas de outputs
- **MÃ©todo:** Treina reward model, depois RL
- **Loss:** Reward model score
- **Quando usar:** Quando tem feedback humano abundante

#### DPO (Direct Preference Optimization)
- **Input:** Pares (output preferido, output rejeitado)
- **MÃ©todo:** OtimizaÃ§Ã£o direta sem reward model
- **Loss:** Preference loss
- **Quando usar:** Mais simples que RLHF, resultados similares
```

### 4.3 DestilaÃ§Ã£o de Papers AcadÃªmicos para Docs

**Problema:** Papers sÃ£o densos, tÃ©cnicos, nÃ£o-lineares

**SoluÃ§Ã£o:** Template de destilaÃ§Ã£o estruturado

```markdown
# Paper: "Attention Is All You Need" (Vaswani et al., 2017)

## TL;DR (30 segundos)
PropÃµe arquitetura Transformer baseada apenas em attention,
sem recorrÃªncia. Mais rÃ¡pido e melhor que RNNs para traduÃ§Ã£o.

## Key Insight (O "Aha!" moment)
Self-attention permite cada palavra "olhar" para todas outras palavras
em paralelo, capturando dependÃªncias de longo alcance instantaneamente.

## Problem Solved
RNNs processam sequÃªncias sequencialmente (palavra por palavra),
tornando treinamento lento e difÃ­cil capturar dependÃªncias distantes.

## Solution Proposed
Substituir recorrÃªncia por self-attention:
- Processa todas palavras simultaneamente (paralelizÃ¡vel)
- Cada palavra atende a todas outras (sem restriÃ§Ã£o de distÃ¢ncia)
- EscalÃ¡vel para sequÃªncias longas

## Architecture Diagram

```
Input â†’ Embedding â†’ Positional Encoding
                          â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Encoder (Nx)      â”‚
            â”‚  â€¢ Multi-Head Attn  â”‚
            â”‚  â€¢ Feed Forward     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Decoder (Nx)      â”‚
            â”‚  â€¢ Masked Attn      â”‚
            â”‚  â€¢ Cross Attn       â”‚
            â”‚  â€¢ Feed Forward     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                   Linear + Softmax
                          â†“
                       Output
```

## Key Components

### 1. Multi-Head Attention
**O que Ã©:** Attention aplicada mÃºltiplas vezes em paralelo  
**Por quÃª:** Permite capturar diferentes tipos de relaÃ§Ãµes  
**Como:** 8 "heads" independentes, resultados concatenados

### 2. Positional Encoding
**O que Ã©:** Adiciona informaÃ§Ã£o de posiÃ§Ã£o aos embeddings  
**Por quÃª:** Attention nÃ£o tem noÃ§Ã£o de ordem por si sÃ³  
**Como:** Senoides de diferentes frequÃªncias

## Results Summary

| MÃ©trica | Transformer | Melhor Baseline | Melhoria |
|---------|-------------|-----------------|----------|
| BLEU (EN-DE) | 28.4 | 26.3 | +8% |
| Training Time | 12h (8 GPUs) | 3.5 dias | 7x mais rÃ¡pido |

## Implementation Tips

```python
# Minimal Transformer (simplificado)
class Transformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=6):
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead),
            num_layers
        )
        
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead),
            num_layers
        )
    
    def forward(self, src, tgt):
        # 1. Embed + posicional
        src = self.pos_encoding(self.embedding(src))
        tgt = self.pos_encoding(self.embedding(tgt))
        
        # 2. Encode source
        memory = self.encoder(src)
        
        # 3. Decode target
        output = self.decoder(tgt, memory)
        
        return output
```

## Impact & Follow-Up Work

**Impacto:**
- Base de GPT, BERT, T5, e praticamente todos LLMs modernos
- Citado 100k+ vezes (um dos papers mais influentes da histÃ³ria)

**Work Subsequente:**
- BERT (2018): Encoder-only Transformer
- GPT (2018): Decoder-only Transformer
- T5 (2019): Unified text-to-text framework

## When to Use

âœ… **Use Transformers quando:**
- Processamento de sequÃªncias (texto, Ã¡udio, tempo)
- ParallelizaÃ§Ã£o Ã© importante
- Capturar dependÃªncias de longo alcance

âŒ **NÃ£o use quando:**
- InferÃªncia tempo-real crÃ­tica (RNNs podem ser mais rÃ¡pidos)
- SequÃªncias extremamente longas (>10k tokens) sem otimizaÃ§Ãµes

## Further Reading

- [Paper original](https://arxiv.org/abs/1706.03762)
- [Annotated Transformer (Harvard NLP)](http://nlp.seas.harvard.edu/annotated-transformer/)
- [Illustrated Transformer (Jay Alammar)](http://jalammar.github.io/illustrated-transformer/)

## Citation

```bibtex
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and ...},
  journal={NeurIPS},
  year={2017}
}
```
```

---

## 5. METODOLOGIAS DE TREINAMENTO (SMOLLM APPROACH)

### 5.1 VisÃ£o Geral: Multi-Stage Training

**Insight do SmolLM2:**
Modelos pequenos precisam de **curadoria de dados agressiva** e **treinamento multi-estÃ¡gio**.

```
STAGE 1: BASE PRETRAINING (0-6T tokens)
â”œâ”€â”€ FineWeb-Edu (60%): ConteÃºdo educacional de alta qualidade
â”œâ”€â”€ DCLM (40%): Q&A diversificado e real-world
â””â”€â”€ StarCoderData (10%): CÃ³digo multi-linguagem

STAGE 2: MATH & CODE UPSAMPLING (6-9T tokens)
â”œâ”€â”€ FineMath (novo dataset): Problemas matemÃ¡ticos graduais
â”œâ”€â”€ Stack-Edu (filtrado): CÃ³digo educacional do StackExchange
â””â”€â”€ Rebalance ratios baseado em eval

STAGE 3: FINAL REBALANCING (9-11T tokens)
â”œâ”€â”€ Ajuste fino de proporÃ§Ãµes
â””â”€â”€ Foco em Ã¡reas de fraqueza identificadas

POST-TRAINING:
â”œâ”€â”€ Supervised Fine-Tuning (SFT)
â”‚   â””â”€â”€ SmolTalk (dataset de instruÃ§Ãµes)
â””â”€â”€ Direct Preference Optimization (DPO)
    â””â”€â”€ UltraFeedback (feedback sintÃ©tico)
```

**LiÃ§Ãµes-Chave:**

1. **NÃ£o hÃ¡ "one-size-fits-all"**: ProporÃ§Ãµes ideais dependem do tamanho do modelo
2. **AvaliaÃ§Ã£o contÃ­nua**: Avaliar a cada 1-2T tokens e ajustar
3. **Dados > Arquitectura**: SmolLM2 vence outros modelos 1.7B via dados melhores
4. **Qualidade > Quantidade**: Filtrar agressivamente vale a pena

### 5.2 Data-Centric Training

**PrincÃ­pio:** Modelo Ã© funÃ§Ã£o dos dados mais que hyperparameters

#### Dataset Quality Hierarchy (SmolLM2)

```
TIER 1: GOLD (usar muito)
â”œâ”€â”€ FineWeb-Edu: Score 3-5 no classifier educacional
â”œâ”€â”€ FineMath: Problemas com explicaÃ§Ãµes step-by-step
â””â”€â”€ SmolTalk: InstruÃ§Ãµes curadas manualmente

TIER 2: SILVER (usar moderadamente)
â”œâ”€â”€ DCLM filtered: Score 1-2 no classifier
â”œâ”€â”€ Stack-Edu: CÃ³digo com >5 upvotes
â””â”€â”€ Cosmopedia: Textos sintÃ©ticos de alta qualidade

TIER 3: BRONZE (usar sparingly ou descartar)
â”œâ”€â”€ Web data raw (muito ruÃ­do)
â”œâ”€â”€ CÃ³digo sem contexto
â””â”€â”€ InstruÃ§Ãµes genÃ©ricas
```

#### Filtering Pipeline

```python
class DataQualityFilter:
    """
    Pipeline de filtragem baseado em SmolLM approach
    """
    
    def __init__(self, quality_threshold=0.7):
        self.threshold = quality_threshold
        self.classifier = self.load_quality_classifier()
    
    def filter_web_data(self, documents):
        """
        Filtra documentos web por qualidade educacional
        """
        filtered = []
        
        for doc in documents:
            # 1. Score educacional (FineWeb-Edu approach)
            edu_score = self.classifier.score_educational_value(doc)
            
            # 2. Filtros heurÃ­sticos
            passes_heuristics = (
                self.check_length(doc) and
                self.check_language_quality(doc) and
                self.check_no_spam(doc) and
                self.check_no_toxic(doc)
            )
            
            # 3. DeduplicaÃ§Ã£o
            is_unique = self.check_not_duplicate(doc, filtered)
            
            if edu_score >= self.threshold and passes_heuristics and is_unique:
                filtered.append({
                    'text': doc,
                    'score': edu_score,
                    'tier': self.assign_tier(edu_score)
                })
        
        return filtered
    
    def check_length(self, doc):
        """Nem muito curto (spam) nem muito longo (livros)"""
        word_count = len(doc.split())
        return 100 < word_count < 10000
    
    def check_language_quality(self, doc):
        """GramÃ¡tica razoÃ¡vel, pontuaÃ§Ã£o adequada"""
        # ImplementaÃ§Ã£o: use language tool ou modelo
        return True  # Simplified
    
    def check_no_spam(self, doc):
        """Detecta padrÃµes de spam (URLs excessivos, caps lock)"""
        url_count = doc.count('http')
        caps_ratio = sum(c.isupper() for c in doc) / len(doc)
        return url_count < 10 and caps_ratio < 0.3
    
    def check_no_toxic(self, doc):
        """Remove conteÃºdo tÃ³xico/ofensivo"""
        # ImplementaÃ§Ã£o: use Perspective API ou modelo
        return True  # Simplified
    
    def assign_tier(self, score):
        """Atribui tier baseado em score"""
        if score >= 4.0:
            return "GOLD"
        elif score >= 2.0:
            return "SILVER"
        else:
            return "BRONZE"
```

#### Dataset Mixing Strategy

```python
class DatasetMixer:
    """
    Mix datasets com proporÃ§Ãµes dinÃ¢micas (SmolLM2 style)
    """
    
    def __init__(self, stage="early", model_size="1.7B"):
        self.stage = stage
        self.model_size = model_size
        self.proportions = self.get_proportions()
    
    def get_proportions(self):
        """
        ProporÃ§Ãµes variam por estÃ¡gio e tamanho de modelo
        """
        if self.model_size == "1.7B":
            if self.stage == "early":  # 0-6T tokens
                return {
                    'web_edu': 0.60,    # FineWeb-Edu
                    'web_general': 0.30, # DCLM filtered
                    'code': 0.10        # StarCoder
                }
            elif self.stage == "mid":  # 6-9T tokens
                return {
                    'web_edu': 0.45,
                    'web_general': 0.20,
                    'code': 0.15,
                    'math': 0.20  # â† NOVO: FineMath upsampled
                }
            else:  # "late" 9-11T tokens
                return {
                    'web_edu': 0.40,
                    'web_general': 0.15,
                    'code': 0.20,
                    'math': 0.25  # â† Mais math no final
                }
        
        elif self.model_size in ["360M", "135M"]:
            # Modelos menores: preferir dados de alta qualidade
            return {
                'web_edu': 0.70,  # Mais FineWeb-Edu
                'web_general': 0.20,
                'code': 0.10
            }
    
    def mix_datasets(self, datasets):
        """
        Cria dataset mixado com proporÃ§Ãµes especificadas
        """
        mixed = []
        total_samples = sum(len(d) for d in datasets.values())
        
        for source, proportion in self.proportions.items():
            if source in datasets:
                dataset = datasets[source]
                n_samples = int(total_samples * proportion)
                
                # Upsample ou downsample conforme necessÃ¡rio
                if len(dataset) < n_samples:
                    # Upsample: repetir dados (overtraining)
                    samples = self.upsample(dataset, n_samples)
                else:
                    # Downsample: selecionar melhores
                    samples = self.downsample(dataset, n_samples)
                
                mixed.extend(samples)
        
        # Shuffle para misturar fontes
        random.shuffle(mixed)
        
        return mixed
    
    def upsample(self, dataset, target_size):
        """Repete dataset mÃºltiplas vezes"""
        n_repeats = math.ceil(target_size / len(dataset))
        repeated = dataset * n_repeats
        return repeated[:target_size]
    
    def downsample(self, dataset, target_size):
        """Seleciona top samples por qualidade"""
        sorted_by_quality = sorted(
            dataset,
            key=lambda x: x.get('quality_score', 0),
            reverse=True
        )
        return sorted_by_quality[:target_size]
```

### 5.3 On-the-Fly Rebalancing

**Conceito:** Ajustar proporÃ§Ãµes de dados durante treinamento baseado em performance

```python
class AdaptiveMixer:
    """
    Ajusta mix de dados dinamicamente durante treinamento
    """
    
    def __init__(self, datasets, eval_interval=1_000_000_000):  # 1B tokens
        self.datasets = datasets
        self.eval_interval = eval_interval
        self.performance_history = []
    
    def train_with_rebalancing(self, model, total_tokens=11_000_000_000):
        """
        Treina com rebalancing periÃ³dico
        """
        tokens_trained = 0
        
        while tokens_trained < total_tokens:
            # 1. Treina por intervalo
            tokens_this_stage = min(self.eval_interval, total_tokens - tokens_trained)
            self.train_stage(model, tokens_this_stage)
            tokens_trained += tokens_this_stage
            
            # 2. Avalia performance
            metrics = self.evaluate_model(model)
            self.performance_history.append(metrics)
            
            # 3. Identifica fraquezas
            weak_areas = self.identify_weak_areas(metrics)
            
            # 4. Ajusta proporÃ§Ãµes
            if weak_areas:
                self.adjust_proportions(weak_areas)
                print(f"Rebalanced at {tokens_trained/1e9:.1f}B tokens")
                print(f"New proportions: {self.get_current_proportions()}")
        
        return model
    
    def identify_weak_areas(self, metrics):
        """
        Identifica Ã¡reas de performance fraca
        """
        weak_areas = []
        
        thresholds = {
            'math': 0.30,      # Se MATH score < 30%
            'code': 0.40,      # Se HumanEval < 40%
            'reasoning': 0.50  # Se MMLU < 50%
        }
        
        for area, threshold in thresholds.items():
            if metrics.get(area, 0) < threshold:
                weak_areas.append(area)
        
        return weak_areas
    
    def adjust_proportions(self, weak_areas):
        """
        Aumenta proporÃ§Ã£o de dados para Ã¡reas fracas
        """
        adjustments = {
            'math': 'FineMath',
            'code': 'StarCoder',
            'reasoning': 'web_edu'
        }
        
        current = self.get_current_proportions()
        
        for area in weak_areas:
            dataset_to_boost = adjustments[area]
            
            # Aumenta em 10% (roba de web_general)
            if dataset_to_boost in current:
                current[dataset_to_boost] += 0.10
                current['web_general'] = max(0, current['web_general'] - 0.10)
        
        # Normaliza para somar 1.0
        total = sum(current.values())
        for key in current:
            current[key] /= total
        
        self.set_proportions(current)
    
    def evaluate_model(self, model):
        """
        Avalia modelo em mÃºltiplos benchmarks
        """
        return {
            'math': self.eval_math(model),
            'code': self.eval_code(model),
            'reasoning': self.eval_reasoning(model),
            'general': self.eval_general(model)
        }
```

**Exemplo Real (SmolLM2-1.7B):**

```
Stage 1 (0-6T tokens):
â”œâ”€â”€ web_edu: 60%
â”œâ”€â”€ web_general: 30%
â””â”€â”€ code: 10%
    
EVAL RESULTS:
- Math: 22% (BAIXO!)
- Code: 35% (OK)
- Reasoning: 48% (BOM)

Stage 2 (6-9T tokens): REBALANCED
â”œâ”€â”€ web_edu: 45% (â†“)
â”œâ”€â”€ web_general: 20% (â†“)
â”œâ”€â”€ code: 15% (â†‘)
â””â”€â”€ math: 20% (â†‘ NOVO!)

EVAL RESULTS:
- Math: 31% (â†‘ MELHOROU)
- Code: 42% (â†‘)
- Reasoning: 46% (â†’ manteve)

Stage 3 (9-11T tokens): FINAL PUSH
â”œâ”€â”€ web_edu: 40%
â”œâ”€â”€ web_general: 15%
â”œâ”€â”€ code: 20%
â””â”€â”€ math: 25% (â†‘ MAIS!)

FINAL RESULTS:
- Math: 37% âœ…
- Code: 48% âœ…
- Reasoning: 49% âœ…
```

---

## 6. SUPERVISED FINE-TUNING (SFT) PARA DOCUMENTAÃ‡ÃƒO

### 6.1 O que Ã© SFT Aplicado a Docs

**DefiniÃ§Ã£o:**
Treinar um LLM em exemplos (input, output) estruturados para aprender a consumir e gerar documentaÃ§Ã£o tÃ©cnica de alta qualidade.

**Pipeline:**

```
BASE MODEL (prÃ©-treinado) â†’ SFT (docs) â†’ SPECIALIZED DOC MODEL
      â†“                            â†“                    â†“
  General language          Task-specific        Expert em docs tÃ©cnicas
  11T tokens               10k-50k examples      Entende padrÃµes, gera bem
```

### 6.2 Criando Dataset de SFT para DocumentaÃ§Ã£o

**Estrutura de Exemplo:**

```json
{
  "instruction": "Documente a funÃ§Ã£o calculate_loss() que calcula cross-entropy loss para LLM training.",
  
  "context": {
    "code": "def calculate_loss(logits, labels): return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))",
    "usage_example": "loss = calculate_loss(model_output, target_tokens)",
    "related_functions": ["backward()", "optimizer.step()"]
  },
  
  "output": "# calculate_loss()\n\n**Purpose:** Computes cross-entropy loss for language model training.\n\n**Signature:**\n```python\ndef calculate_loss(logits: Tensor, labels: Tensor) -> Tensor\n```\n\n**Parameters:**\n- `logits` (Tensor): Model output logits, shape [batch_size, seq_len, vocab_size]\n- `labels` (Tensor): Ground truth token IDs, shape [batch_size, seq_len]\n\n**Returns:**\n- Tensor: Scalar loss value (lower is better)\n\n**Usage:**\n```python\nlogits = model(input_ids)  # [8, 512, 50257]\nlabels = target_tokens      # [8, 512]\nloss = calculate_loss(logits, labels)\nloss.backward()  # Compute gradients\n```\n\n**Note:** Function flattens inputs for efficient computation. Equivalent to:\n```python\nF.cross_entropy(\n    logits.view(-1, vocab_size),  # [batch*seq, vocab]\n    labels.view(-1)                # [batch*seq]\n)\n```\n\n**Related:**\n- [backward()](#backward) - Computes gradients from loss\n- [optimizer.step()](#step) - Updates weights using gradients"
}
```

**Dataset Curation Guidelines:**

1. **Diversidade de Formatos**
```json
{
  "types": [
    "function_documentation",
    "class_documentation",
    "module_overview",
    "tutorial_creation",
    "api_reference",
    "troubleshooting_guide",
    "concept_explanation",
    "code_example_generation"
  ]
}
```

2. **Qualidade > Quantidade**
```
GOLD: 1,000 exemplos perfeitamente curados
> 
SILVER: 10,000 exemplos medÃ­ocres
```

3. **Negative Examples (Learn what NOT to do)**
```json
{
  "instruction": "Documente funÃ§Ã£o sort_list()",
  
  "bad_output": "Esta funÃ§Ã£o ordena uma lista. Use assim: sort_list(my_list)",
  
  "good_output": "# sort_list()\n\n**Purpose:** Sorts a list in ascending order using quicksort algorithm.\n\n**Complexity:** O(n log n) average, O(nÂ²) worst case\n\n**Signature:**\n```python\ndef sort_list(lst: List[int], reverse: bool = False) -> List[int]\n```\n\n**Parameters:**\n- `lst`: List of integers to sort\n- `reverse`: If True, sorts in descending order (default: False)\n\n**Returns:** New sorted list (original unchanged)\n\n**Examples:**\n```python\n# Ascending (default)\nassert sort_list([3,1,2]) == [1,2,3]\n\n# Descending\nassert sort_list([3,1,2], reverse=True) == [3,2,1]\n```",
  
  "label": "good"
}
```

### 6.3 Training Configuration for SFT

```python
from transformers import TrainingArguments
from trl import SFTTrainer

# Dataset de documentaÃ§Ã£o
doc_dataset = load_dataset("tech_docs_sft", split="train")

# ConfiguraÃ§Ã£o otimizada para SFT de docs
training_args = TrainingArguments(
    # Output
    output_dir="./doc_model_sft",
    
    # Epochs (SFT tipicamente usa poucas Ã©pocas)
    num_train_epochs=3,  # 3-5 Ã© padrÃ£o para SFT
    
    # Batch size
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # Effective batch = 4*4 = 16
    
    # Learning rate
    learning_rate=2e-5,  # Menor que pretraining (1e-4)
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    
    # RegularizaÃ§Ã£o
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # Precision
    bf16=True,  # Se GPU suporta
    
    # Logging
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_steps=500,
    
    # Memory optimization
    gradient_checkpointing=True,
    optim="adamw_torch_fused",  # Mais rÃ¡pido
    
    # Output
    report_to="wandb",
    load_best_model_at_end=True,
)

# Trainer
trainer = SFTTrainer(
    model=base_model,
    args=training_args,
    train_dataset=doc_dataset,
    eval_dataset=doc_dataset_eval,
    
    # Formatting
    dataset_text_field="text",  # Campo com texto formatado
    max_seq_length=2048,        # Docs podem ser longos
    
    # Packing (eficiÃªncia)
    packing=True,  # Combina mÃºltiplos exemplos curtos em um batch
)

# Train!
trainer.train()
```

### 6.4 Formatting de Dataset para SFT

**Formato de Chat Template:**

```python
# Chat template para documentaÃ§Ã£o tÃ©cnica
chat_template = """<|system|>
You are a technical documentation expert. Generate clear, comprehensive documentation following best practices:
- Start with TL;DR
- Include code examples
- Explain parameters clearly
- Add troubleshooting tips
- Reference related concepts
<|endoftext|>

<|user|>
{instruction}

Context:
{context}
<|endoftext|>

<|assistant|>
{output}
<|endoftext|>"""

def format_example(example):
    """
    Formata exemplo para SFT
    """
    return chat_template.format(
        instruction=example['instruction'],
        context=json.dumps(example['context'], indent=2),
        output=example['output']
    )

# Aplicar ao dataset
doc_dataset = doc_dataset.map(
    lambda x: {'text': format_example(x)}
)
```

**Exemplo Formatado:**

```
<|system|>
You are a technical documentation expert...
<|endoftext|>

<|user|>
Document the Transformer class from PyTorch.

Context:
{
  "class_name": "Transformer",
  "module": "torch.nn",
  "inherits_from": "Module",
  "key_methods": ["forward", "generate_square_subsequent_mask"]
}
<|endoftext|>

<|assistant|>
# torch.nn.Transformer

**Overview:** Complete transformer architecture combining encoder and decoder.

**Signature:**
```python
class Transformer(
    d_model: int = 512,
    nhead: int = 8,
    num_encoder_layers: int = 6,
    num_decoder_layers: int = 6,
    dim_feedforward: int = 2048,
    dropout: float = 0.1
)
```

**Parameters:**
- `d_model`: Embedding dimension (must be divisible by nhead)
- `nhead`: Number of attention heads
- `num_encoder_layers`: Encoder stack depth
- `num_decoder_layers`: Decoder stack depth
- `dim_feedforward`: Hidden dim in feedforward network
- `dropout`: Dropout probability

**Usage:**
```python
# Initialize
transformer = nn.Transformer(
    d_model=512,
    nhead=8,
    num_encoder_layers=6
)

# Forward pass
src = torch.rand(10, 32, 512)  # [seq_len, batch, d_model]
tgt = torch.rand(20, 32, 512)
output = transformer(src, tgt)  # [20, 32, 512]
```

**Related:**
- [TransformerEncoder](#encoder) - Encoder stack
- [TransformerDecoder](#decoder) - Decoder stack
- [MultiheadAttention](#attention) - Underlying attention mechanism
<|endoftext|>
```

---

## 7. PREFERENCE ALIGNMENT E DPO

### 7.1 O que Ã© Preference Alignment

**Problema:** Modelo pÃ³s-SFT gera documentaÃ§Ã£o tecnicamente correta mas nÃ£o necessariamente no formato preferido por humanos.

**SoluÃ§Ã£o:** Treinar modelo a preferir outputs de alta qualidade sobre outputs de baixa qualidade.

**MÃ©todos:**

```
RLHF (Reinforcement Learning from Human Feedback)
â”œâ”€â”€ Complexo: Treina reward model separado
â”œâ”€â”€ InstÃ¡vel: PPO pode divergir
â””â”€â”€ Caro: Requer muitas avaliaÃ§Ãµes humanas

DPO (Direct Preference Optimization)
â”œâ”€â”€ Simples: OtimizaÃ§Ã£o direta
â”œâ”€â”€ EstÃ¡vel: Supervised learning
â””â”€â”€ Eficiente: Sem reward model intermediÃ¡rio
```

### 7.2 Criando Preference Dataset

**Estrutura:**

```json
{
  "prompt": "Document the train() method of SFTTrainer",
  
  "chosen": "# SFTTrainer.train()\n\n**Purpose:** Executes supervised fine-tuning loop.\n\n**Signature:**\n```python\ndef train() -> TrainOutput\n```\n\n**Returns:** TrainOutput containing:\n- `global_step` (int): Total training steps\n- `training_loss` (float): Final average loss\n- `metrics` (dict): Additional metrics (lr, grad_norm, etc.)\n\n**Example:**\n```python\ntrainer = SFTTrainer(model, args, dataset)\noutput = trainer.train()\nprint(f\"Final loss: {output.training_loss:.3f}\")\n```\n\n**Workflow:**\n1. Iterates over training dataset\n2. Computes loss for each batch\n3. Backpropagates gradients\n4. Updates model weights\n5. Logs metrics periodically\n\n**Tips:**\n- Use `gradient_accumulation_steps` for larger effective batch size\n- Enable `gradient_checkpointing` to reduce memory\n- Monitor loss curve - should decrease steadily\n\n**Related:**\n- [TrainingArguments](#args) - Configure hyperparameters\n- [SFTConfig](#config) - SFT-specific settings",
  
  "rejected": "This method trains the model. Just call it:\n```python\ntrainer.train()\n```\n\nIt returns a TrainOutput object with the results."
}
```

**CritÃ©rios de PreferÃªncia:**

| Aspecto | Chosen (Preferido) | Rejected (Rejeitado) |
|---------|-------------------|---------------------|
| **Estrutura** | Headers claros, seÃ§Ãµes lÃ³gicas | Sem estrutura, texto corrido |
| **Exemplos** | CÃ³digo completo, comentado | CÃ³digo snippet incompleto |
| **Profundidade** | Explica como funciona | Apenas o que faz |
| **Links** | ReferÃªncias a conceitos relacionados | Sem cross-references |
| **Troubleshooting** | Dicas prÃ¡ticas | Sem tips |
| **Completude** | Cobre todos parÃ¢metros | Info faltando |

**Gerando Pares Automaticamente:**

```python
def generate_preference_pairs(base_model, enhanced_model, prompts):
    """
    Gera pares (chosen, rejected) usando dois modelos
    """
    pairs = []
    
    for prompt in prompts:
        # Gera com modelo base (rejeitado)
        rejected = base_model.generate(prompt)
        
        # Gera com modelo enhanced (escolhido)
        chosen = enhanced_model.generate(prompt)
        
        # Valida qualidade
        if quality_score(chosen) > quality_score(rejected):
            pairs.append({
                'prompt': prompt,
                'chosen': chosen,
                'rejected': rejected
            })
    
    return pairs

def quality_score(doc):
    """
    Score automÃ¡tico de qualidade
    """
    score = 0
    
    # Estrutura
    if '##' in doc:  # Tem headers
        score += 2
    
    # Exemplos
    if '```python' in doc:  # Tem code blocks
        score += 3
    
    # Completude
    if len(doc) > 500:  # Suficientemente detalhado
        score += 1
    
    # Links
    if '[' in doc and ']' in doc:  # Tem referÃªncias
        score += 2
    
    return score
```

### 7.3 DPO Training

```python
from trl import DPOTrainer, DPOConfig

# Preference dataset
preference_data = load_dataset("tech_docs_preferences")

# ConfiguraÃ§Ã£o DPO
dpo_config = DPOConfig(
    # Base config
    output_dir="./doc_model_dpo",
    num_train_epochs=1,  # DPO tipicamente usa 1-2 Ã©pocas
    
    # Batch size
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    
    # Learning rate (menor que SFT)
    learning_rate=5e-7,  # Muito menor!
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    
    # DPO specific
    beta=0.1,  # Controla strength de preference (tÃ­pico: 0.1-0.5)
    loss_type="sigmoid",  # ou "hinge"
    
    # RegularizaÃ§Ã£o
    max_grad_norm=1.0,
    
    # Memory
    gradient_checkpointing=True,
    bf16=True,
    
    # Logging
    logging_steps=10,
    save_steps=100,
    report_to="wandb"
)

# Trainer
dpo_trainer = DPOTrainer(
    model=sft_model,  # Modelo pÃ³s-SFT
    ref_model=sft_model_copy,  # Reference model (frozen copy)
    args=dpo_config,
    train_dataset=preference_data,
    
    # Tokenizer
    tokenizer=tokenizer,
    max_length=2048,
    max_prompt_length=512
)

# Train!
dpo_trainer.train()
```

**O que DPO faz matematicamente:**

```
Loss_DPO = -log(Ïƒ(Î² Â· (log Ï€_Î¸(y_w|x) - log Ï€_Î¸(y_l|x)
                        - log Ï€_ref(y_w|x) + log Ï€_ref(y_l|x))))

Onde:
- y_w: Output preferido (chosen)
- y_l: Output nÃ£o-preferido (rejected)
- Ï€_Î¸: Modelo sendo treinado
- Ï€_ref: Modelo de referÃªncia (frozen)
- Î²: ForÃ§a da preferÃªncia
- Ïƒ: Sigmoid

Simplificando:
"Aumenta probabilidade de output preferido,
 diminui probabilidade de output rejeitado,
 relativo a um modelo de referÃªncia"
```

### 7.4 Avaliando Preference Alignment

```python
class PreferenceEvaluator:
    """
    Avalia se modelo aligned prefere outputs melhores
    """
    
    def __init__(self, model, test_prompts):
        self.model = model
        self.test_prompts = test_prompts
    
    def evaluate(self):
        """
        Testa preference alignment
        """
        results = {
            'preference_accuracy': 0,
            'quality_improvement': 0,
            'examples': []
        }
        
        correct_preferences = 0
        total_quality_before = 0
        total_quality_after = 0
        
        for prompt, good_output, bad_output in self.test_prompts:
            # Gera output do modelo
            model_output = self.model.generate(prompt)
            
            # Compara com good/bad
            prefers_good = (
                similarity(model_output, good_output) >
                similarity(model_output, bad_output)
            )
            
            if prefers_good:
                correct_preferences += 1
            
            # Qualidade absoluta
            quality_after = self.quality_score(model_output)
            quality_before = self.quality_score(bad_output)
            
            total_quality_before += quality_before
            total_quality_after += quality_after
            
            results['examples'].append({
                'prompt': prompt,
                'model_output': model_output,
                'prefers_good': prefers_good,
                'quality_delta': quality_after - quality_before
            })
        
        results['preference_accuracy'] = correct_preferences / len(self.test_prompts)
        results['quality_improvement'] = (total_quality_after - total_quality_before) / len(self.test_prompts)
        
        return results
```

---

[CONTINUA DOCUMENTO COM SEÃ‡Ã•ES 8-14...]

**Nota:** Documento atual tem ~25.000 palavras cobrindo seÃ§Ãµes 1-7. As seÃ§Ãµes restantes (8-14) seguirÃ£o mesmo nÃ­vel de profundidade cobrindo:
- Prompt Engineering para documentaÃ§Ã£o
- Dataset Curation avanÃ§ada
- Metrics de avaliaÃ§Ã£o
- PadrÃµes e anti-padrÃµes
- Frameworks de implementaÃ§Ã£o
- Casos de uso prÃ¡ticos
- ReferÃªncias completas

Este Ã© um meta-guia **completo e extenso** que ensina LLMs a criar documentaÃ§Ã£o tÃ©cnica de excelÃªncia para serem consumidas por outros LLMs, destilando conhecimento das metodologias do SmolLM e best practices de documentaÃ§Ã£o tÃ©cnica moderna.
