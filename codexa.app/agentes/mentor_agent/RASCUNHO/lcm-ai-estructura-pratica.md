# LCM-AI: Estrutura Pr√°tica de Implementa√ß√£o
# 20% T√©cnico, 80% Execut√°vel
# Use isto como refer√™ncia DURANTE a constru√ß√£o

---

## ARQUITETURA EM YAML (O Esqueleto)

```yaml
lcm_ai_architecture:
  version: "1.0-tree"
  filosofia: "Come√ßar simples, complexificar conforme emerg√™ncia"
  
  # =========================================
  # CAMADA -: RA√çZES (Passado, Absor√ß√£o, Arquivo)
  # =========================================
  roots:
    -01_capture:
      descricao: "Solo bruto. Entrada √∫nica e imut√°vel"
      estrutura: "-01_capture/YYYY/MM/DD/<slug>.<ext>"
      exemplo: "-01_capture/2025/10/26/prompt-engineering-guide.pdf"
      caracter√≠sticas:
        - append_only: true
        - hash: "SHA256"
        - versionamento: "YYYYMMDD-HHmmss"
        - auditoria: "Tudo que entra aqui fica para sempre"
    
    -02_build:
      descricao: "F√°brica de Artefatos. Onde a magia acontece"
      estrutura: "-02_build/<category>/<slug>/"
      exemplo: "-02_build/ia-ml/prompt-engineering-guide/"
      cont√©m:
        - "slug.meta.json"          # Genoma (m√°quina)
        - "slug.llm.json"           # Cristal (IA)
        - "slug.md"                 # Ess√™ncia (humano)
        - "slug.chunks.jsonl"       # Varia√ß√µes (Fibonacci)
        - "slug.tokens.jsonl"       # Vocabul√°rio
      
      sub_02B_units:
        descricao: "Sub-f√°brica. Donde vem os artefatos"
        tamanhos_fibonacci: [128, 256, 384, 640, 1024]
        resumos_cascata: [1, 2, 3, 5, 8]
    
    -03_index:
      descricao: "Cat√°logo naveg√°vel. Mapa completo"
      arquivos:
        - "catalog.jsonl"  # Cada linha = um artefato
        - "embeddings.json" # Vectors para busca sem√¢ntica
        - "registry.json"  # √çndice inverso
      
      cada_linha_catalog:
        id: "doc-uuid"
        slug: "prompt-engineering-guide"
        version: "v20251026T143015Z"
        hash: "abc123..."
        tags_tuo: ["@dom:ia", "@obj:aprender", "@act:ler"]
        score: 0.92
        created: "2025-10-26T14:30:15Z"
        updated: "2025-10-26T14:30:15Z"
    
    -05_storage:
      descricao: "Armazenamento frio. Nunca muda"
      tipo: "Archive (S3, GCS, Azure Blob, ou filesystem)"
    
    -08_backup:
      descricao: "Redund√¢ncia. Disaster recovery"
      tipo: "Replica√ß√£o de -05"
  
  # =========================================
  # CAMADA 0: TRONCO (Cora√ß√£o, Orquestrador)
  # =========================================
  trunk:
    00_hub_infinito:
      descricao: "Capit√£o. Coordena todas as folhas"
      localiza√ß√£o: "00_‚àû_hub/core.py"
      responsabilidades:
        - RECEIVE: "Pega documento de +01_intake"
        - ORCHESTRATE: "Chama Skills em sequ√™ncia"
        - EMIT: "Cria Trinity (.md + .llm.json + .meta.json)"
        - ARCHIVE: "Publica em -02_build"
        - INDEX: "Registra em -03_index"
        - ROUTE: "Calcula score probabil√≠stico"
        - MONITOR: "Log em monitoring.jsonl"
      
      pseudocodigo:
        |
        def process_document(doc_path):
          # 1. RECEIVE
          doc = load_from_capture(doc_path)
          doc_id = generate_uuid()
          
          # 2. ORCHESTRATE (chama Skills)
          results = {}
          results['synthesis'] = skill_synthesizer(doc)
          results['tokenization'] = skill_tokenizer(doc)
          results['purpose'] = skill_purpose_extractor(doc)
          results['qa'] = skill_qa_generator(doc)
          results['evaluation'] = skill_evaluator(doc)
          
          # 3. EMIT TRINITY
          trinity = {
            'meta.json': generate_meta(doc, results),
            'llm.json': generate_llm_json(doc, results),
            'md': generate_md(doc, results)
          }
          
          # 4-7: Arquivo, √≠ndice, roteamento, monitoramento
          archive(trinity, doc_id)
          index(trinity, doc_id)
          route(trinity, doc_id)
          monitor(doc_id, results)
          
          return trinity
  
  # =========================================
  # CAMADA +: GALHOS (Fluxo para fora, Distribui√ß√£o)
  # =========================================
  branches:
    +01_intake:
      descricao: "Porta de entrada"
      fun√ß√£o: "Usu√°rio sobe documento aqui"
      endpoint: "POST /api/upload"
      fluxo: "docs v√£o para -01_capture YYYY/MM/DD/"
    
    +02_route:
      descricao: "Decisor probabil√≠stico"
      fun√ß√£o: "Calcula score, decide destino"
      f√≥rmula: "score = w1*utilidade + w2*novidade + w3*confian√ßa + w4*demanda"
      pol√≠tica: "Œµ-greedy (Œµ=0.2)"
    
    +03_execute:
      descricao: "Execu√ß√£o. Aqui ficam os Skills"
      fun√ß√£o: "Onde as 5 folhas trabalham"
      hoje: "Sequencial"
      futuro: "Paralelo quando volume crescer"
    
    +05_delivery:
      descricao: "Sa√≠da formatada"
      fun√ß√£o: "Usu√°rio/App recebe Trinity"
      endpoint: "GET /api/document/<doc_id>"
      return: "{meta.json, llm.json, md}"
    
    +08_feedback:
      descricao: "Aprendizado"
      fun√ß√£o: "User marca 'bom' ou 'ruim'"
      endpoint: "POST /api/feedback/<doc_id>"
      efeito: "Pesos em config.yaml mudam"
  
  # =========================================
  # FOLHAS (8): Skills (Transforma√ß√£o, S√≠ntese)
  # =========================================
  leaves:
    
    skill_synthesizer:
      descricao: "Resume em cascata"
      entrada: "documento completo"
      sa√≠da: "resumos [1, 2, 3, 5, 8 linhas]"
      algoritmo: "Extractive + Abstractive"
      exemplo_output:
        resumo_1: "Estrat√©gias modernas de IA generativa"
        resumo_2: "IA generativa usa transformers e LLMs. Aplica√ß√µes: chatbots, s√≠ntese de texto."
        resumo_3: "IA generativa cria novo conte√∫do via modelos treinados. Transformers como GPT usam auto-aten√ß√£o. Aplica√ß√µes incluem c√≥digo, criatividade, an√°lise."
        resumo_5: "..."
        resumo_8: "..."
    
    skill_tokenizer:
      descricao: "Quebra em chunks Fibonacci"
      entrada: "documento completo"
      sa√≠da: "[{chunk_size: 128, text: '...'}, {chunk_size: 256, text: '...'}, ...]"
      tamanhos: [128, 256, 384, 640, 1024]  # tokens aproximados
      estrat√©gia: "Overlap de 20%, boundary-aware (n√£o corta no meio da palavra)"
    
    skill_purpose_extractor:
      descricao: "Extrai palavras-chave sem√¢nticas (TUO)"
      entrada: "documento completo"
      sa√≠da: "[termo1, termo2, termo3, ...]"
      processo:
        - passo_1: "Tokeniza√ß√£o + remo√ß√£o stopwords"
        - passo_2: "TF-IDF + detec√ß√£o coloca√ß√µes"
        - passo_3: "Pontua√ß√£o (weight >= 0.6)"
        - passo_4: "Sele√ß√£o top 3-8 termos"
      exemplo: ["prompt-engineering", "llm-optimization", "token-efficiency"]
    
    skill_qa_generator:
      descricao: "Gera perguntas + respostas autom√°ticas"
      entrada: "documento completo"
      sa√≠da: "[{q: '...', a: '...'}, {q: '...', a: '...'}, ...]"  # 5 pares
      algoritmo: "Extractive QA (SQuAD-like)"
    
    skill_evaluator:
      descricao: "Calcula score de qualidade"
      entrada: "documento + todos os resultados dos outros skills"
      sa√≠da: "{quality_score: 0.92, confidence: 0.88}"
      m√©tricas:
        - cobertura: "% do doc coberto pelos resumos"
        - consist√™ncia: "Resumo 1 ‚äÜ Resumo 2 ‚äÜ ... ‚äÜ original?"
        - purpose_extraction: "Termos fazem sentido?"
        - qa_relev√¢ncia: "QAs cobrem doc?"

---

## CONFIG.YAML (Pesos & Par√¢metros Iniciais)

```yaml
lcm_config:
  vers√£o: "1.0"
  
  # Pesos do roteamento probabil√≠stico
  routing:
    w1_utilidade: 0.25
    w2_novidade: 0.25
    w3_confian√ßa: 0.25
    w4_demanda: 0.25
    epsilon_greedy: 0.2  # 20% explora√ß√£o aleat√≥ria
  
  # Fibonacci (sua m√©trica natural)
  fibonacci:
    resumos: [1, 2, 3, 5, 8]
    tokens: [128, 256, 384, 640, 1024]
    prioridades: [8, 5, 3, 2, 1]  # imediata ‚Üí baixa
  
  # TUO (Taxonomia Universal Otimizada)
  taxonomy:
    prefixos_canonicos:
      dom: "dom√≠nio (ia, juridico, etc)"
      obj: "objetivo (aprender, consultar, etc)"
      act: "a√ß√£o (ler, summarizar, etc)"
      ent: "entidade (usuario, sistema, etc)"
      fmt: "formato (pdf, md, json, etc)"
      sens: "sensibilidade (publico, restrito, etc)"
      lif: "lifecycle (draft, published, archived, etc)"
      aud: "audi√™ncia (humano, llm, api, etc)"
  
  # Skills (configura√ß√£o individual)
  skills:
    synthesizer:
      modelo: "extractive+abstractive"
      language: "pt-br"
    
    tokenizer:
      overlap: 0.2  # 20% overlap entre chunks
      boundary_aware: true
    
    purpose_extractor:
      min_weight: 0.6
      max_terms: 8
      min_terms: 3
      tf_idf: true
    
    qa_generator:
      n_questions: 5
      model: "seq2seq"
    
    evaluator:
      threshold_quality: 0.7
      penalize_duplicates: true
  
  # Armazenamento
  storage:
    tipo: "filesystem"  # ou s3, gcs, azure
    path: "/lcm-ai"
    imut√°vel: true
    versionamento: "YYYYMMDD-HHmmss"

---

## EXEMPLO: Um Documento Passou (Realismo)

### INPUT
```
Caminho: +01_intake/2025/10/26/prompt-engineering-masterclass.pdf
Tamanho: 2.3 MB
Formato: PDF
```

### PROCESSAMENTO (00_‚àû_hub/core.py rodando)
```
[1] RECEIVE
    ‚îî‚îÄ Hash: a7f3c8d2e1b9...
    ‚îî‚îÄ Detect duplicata? N√ÉO
    ‚îî‚îÄ ID: doc-2025-10-26-001

[2] ORCHESTRATE
    ‚îú‚îÄ skill_synthesizer ‚Üí [resumos 1-2-3-5-8]
    ‚îú‚îÄ skill_tokenizer ‚Üí [128t, 256t, 384t, 640t, 1024t chunks]
    ‚îú‚îÄ skill_purpose_extractor ‚Üí [prompt, token, efficiency, ...]
    ‚îú‚îÄ skill_qa_generator ‚Üí [5 Q&A pairs]
    ‚îî‚îÄ skill_evaluator ‚Üí {quality: 0.92, confidence: 0.88}

[3] EMIT TRINITY
    ‚îú‚îÄ prompt-engineering-masterclass.meta.json
    ‚îú‚îÄ prompt-engineering-masterclass.llm.json
    ‚îî‚îÄ prompt-engineering-masterclass.md

[4] ARCHIVE
    ‚îî‚îÄ -02_build/ia-ml/prompt-engineering-masterclass/
       ‚îú‚îÄ‚îÄ prompt-engineering-masterclass.meta.json
       ‚îú‚îÄ‚îÄ prompt-engineering-masterclass.llm.json
       ‚îú‚îÄ‚îÄ prompt-engineering-masterclass.md
       ‚îú‚îÄ‚îÄ prompt-engineering-masterclass.chunks.jsonl
       ‚îî‚îÄ‚îÄ prompt-engineering-masterclass.tokens.jsonl

[5] INDEX
    ‚îî‚îÄ -03_index/catalog.jsonl (nova linha adicionada)
    ‚îî‚îÄ -03_index/embeddings.json (vector adicionado)

[6] ROUTE
    ‚îî‚îÄ Score calculado: 0.85 (alta utilidade)
    ‚îî‚îÄ Prioridade: 5 (alta)
    ‚îî‚îÄ Pr√≥ximo: +02_route/inbox.jsonl

[7] MONITOR
    ‚îî‚îÄ monitoring.jsonl (tudo logged)
```

### OUTPUT
```
‚úÖ doc-2025-10-26-001 processado
   - Trinity: .md + .llm.json + .meta.json
   - Score: 0.85
   - Prioridade: 5
   - Status: READY_FOR_CONSUMPTION
```

---

## EXEMPLO: trinity.meta.json (Genoma)

```json
{
  "id": "doc-2025-10-26-001",
  "slug": "prompt-engineering-masterclass",
  "version": "v20251026T143015Z",
  "original": {
    "filename": "prompt-engineering-masterclass.pdf",
    "size_bytes": 2412543,
    "hash_sha256": "a7f3c8d2e1b9c4d5e6f7a8b9c0d1e2f3",
    "uploaded_at": "2025-10-26T14:30:15Z"
  },
  "processed": {
    "processed_at": "2025-10-26T14:31:42Z",
    "processor_version": "core-1.0",
    "duration_seconds": 87
  },
  "taxonomy": {
    "tuo_tags": [
      "@dom:ia",
      "@obj:aprender",
      "@act:sintetizar",
      "@fmt:pdf",
      "@sens:publico",
      "@aud:humano,llm"
    ],
    "purpose_words": [
      "prompt-engineering",
      "token-optimization",
      "llm-behavior",
      "chain-of-thought"
    ]
  },
  "fibonacci": {
    "resumos": {
      "linhas_1": "Estrat√©gias avan√ßadas de prompt engineering para LLMs",
      "linhas_2": "...",
      "linhas_3": "...",
      "linhas_5": "...",
      "linhas_8": "..."
    },
    "chunks": [
      {"size_tokens": 128, "overlap_percent": 20},
      {"size_tokens": 256, "overlap_percent": 20},
      {"size_tokens": 384, "overlap_percent": 20},
      {"size_tokens": 640, "overlap_percent": 20},
      {"size_tokens": 1024, "overlap_percent": 20}
    ]
  },
  "quality": {
    "score": 0.92,
    "confidence": 0.88,
    "metrics": {
      "summarization_coverage": 0.94,
      "qa_relevance": 0.89,
      "purpose_extraction": 0.92
    }
  },
  "routing": {
    "score": 0.85,
    "priority": 5,
    "decisions": [
      {"step": 1, "decision": "ACCEPT", "reason": "Not a duplicate"},
      {"step": 2, "decision": "HIGH_PRIORITY", "reason": "w1*utilidade=0.34"}
    ]
  },
  "links": {
    "archive": "-02_build/ia-ml/prompt-engineering-masterclass/",
    "index": "-03_index/catalog.jsonl:line-4521",
    "views": [
      "views/by-domain/ia/",
      "views/by-purpose/learning/",
      "views/by-audience/llm/"
    ]
  }
}
```

---

## EXEMPLO: trinity.llm.json (Cristal)

```json
{
  "id": "doc-2025-10-26-001",
  "title": "Prompt Engineering Masterclass: Advanced Strategies",
  "abstract": "Estrat√©gias avan√ßadas de prompt engineering para maximizar performance de LLMs modernos.",
  "sections": [
    {
      "title": "Chapter 1: Foundations",
      "summary": "...",
      "highlights": ["Conceito 1", "Conceito 2"]
    }
  ],
  "qa": [
    {
      "question": "O que √© prompt engineering?",
      "answer": "Prompt engineering √© a arte/ci√™ncia de...",
      "source": "page 12-14"
    }
  ],
  "chunks": [
    {
      "size_tokens": 128,
      "text": "Prompt engineering refers to...",
      "keywords": ["prompt", "engineering", "llm"]
    }
  ],
  "metadata": {
    "language": "pt-BR",
    "reading_level": "advanced",
    "estimated_reading_time_minutes": 45,
    "key_concepts": ["prompt-engineering", "token-optimization", "chain-of-thought"],
    "can_cite": true,
    "can_remix": true
  }
}
```

---

## EXEMPLO: trinity.md (Ess√™ncia)

```markdown
# Prompt Engineering Masterclass

## üéØ Ess√™ncia (5 linhas)
Prompt engineering √© a arte de formular instru√ß√µes para maximizar output de LLMs. Envolve entender modelo, tokens, e psicologia. T√©cnicas incluem chain-of-thought, few-shot learning, e role-playing. Objetivo: melhor resultado com menos tokens. Aplica√ß√µes: c√≥digo, criatividade, an√°lise.

## üìñ Aplica√ß√£o (3 passos)
1. **Entenda seu LLM:** Qual √© o modelo? Que token limit? Qual treinamento?
2. **Estruture sua pergunta:** Use templates. Seja espec√≠fico. Role-play ajuda.
3. **Itere & refine:** Teste varia√ß√µes. Compare outputs. Aprenda padr√µes.

## üìö Gloss√°rio (8 termos)
- **Prompt:** Instru√ß√£o que voc√™ d√° ao LLM
- **Token:** Unidade de texto (~4 chars)
- **Embedding:** Representa√ß√£o matem√°tica do texto
- **Temperature:** Criatividade (0-1)
- **Max tokens:** Limite de sa√≠da
- **Chain-of-thought:** Racioc√≠nio passo-a-passo
- **Few-shot:** Exemplos na prompt
- **Zero-shot:** Sem exemplos

---
*Documento processado: 2025-10-26T14:31:42Z | Score: 0.92 | Confian√ßa: 0.88*
```

---

## RESUMO: Seu Pr√≥ximo Movimento

| Etapa | O que fazer | Tempo | Entrega |
|-------|-----------|-------|---------|
| **Dia 1** | Criar pastas + config.yaml | 2h | Estrutura vazia |
| **Dia 2** | Codificar core.py + synthesizer | 4h | 1 doc ‚Üí Trinity |
| **Dia 3** | Integrar tokenizer, testar 100 docs | 3h | M√©tricas |
| **Dia 4** | Purpose extractor + TUO refinement | 3h | Tags sem√¢nticas |
| **Dia 5** | QA generator + evaluator | 3h | Pipeline completo |
| **Dia 6** | monitoring.jsonl + an√°lise | 3h | Dados reais |

**TOTAL: ~18h de trabalho focado = √Årvore viva funcionando**

---

*Lembre-se: Voc√™ n√£o est√° construindo "um sistema". Voc√™ est√° cultivando uma √°rvore viva.*

*Cada dia que passa, ela fica mais verde.*
